# <!-- Powered by BMAD™ Core -->

## Status
Ready for Review

## Story
**As a** backend developer,
**I want** a Server-Sent Events (SSE) streaming endpoint at `/api/chat/stream`,
**so that** the frontend can receive real-time updates for agent status, message chunks, and retrieval logs progressively

## Acceptance Criteria
1. New endpoint `/api/chat/stream` accepts POST requests with ChatRequest body
2. Endpoint returns `text/event-stream` content type with proper SSE headers
3. Streams four event types: `message_chunk`, `agent_status`, `retrieval_log`, `done`
4. `message_chunk` events contain partial message content (word-by-word streaming)
5. `agent_status` events update agent state (ROUTING, ACTIVE, COMPLETE)
6. `retrieval_log` events send log entries as they're generated
7. `done` event signals stream completion with final metadata
8. Each SSE event follows W3C format: `event: {type}\ndata: {json}\n\n`
9. Stream sends events with 50ms delay between words (configurable)
10. Connection closes cleanly after `done` event
11. Endpoint handles client disconnect gracefully (no errors logged)
12. CORS headers allow frontend origin (localhost:3000 in dev)

## Tasks / Subtasks

- [x] Task 1: Create SSE utility function (AC: 8)
  - [x] Create `src/services/sse_utils.py` file
  - [x] Implement `create_sse_event(event_type: str, data: dict) -> str` function
  - [x] Format output as `event: {type}\ndata: {json_string}\n\n`
  - [x] Handle JSON serialization of data (use Pydantic .model_dump())
  - [x] Add docstring with SSE format explanation
  - [x] Write unit tests for event formatting

- [x] Task 2: Create chat stream endpoint (AC: 1, 2, 12)
  - [x] Add new route in `src/api/routes/chat.py`: `@router.post("/chat/stream")`
  - [x] Accept `ChatRequest` Pydantic model as input
  - [x] Return `StreamingResponse` with `media_type="text/event-stream"`
  - [x] Add headers: `Cache-Control: no-cache`, `X-Accel-Buffering: no` (disable nginx buffering)
  - [x] Add CORS headers for localhost:3000 origin
  - [x] Implement async generator function `event_generator()`
  - [x] Write integration test for endpoint

- [x] Task 3: Implement event generator logic (AC: 3, 4, 5, 6, 7, 9, 10)
  - [x] Create async generator that yields SSE events
  - [x] Step 1: Emit `agent_status` event (Orchestrator: ROUTING)
  - [x] Step 2: Wait 100ms, emit `agent_status` (Orchestrator: IDLE, target agent: ACTIVE)
  - [x] Step 3: Emit `retrieval_log` event (query_analysis)
  - [x] Step 4: Emit `retrieval_log` event (vector_search) - 80% probability
  - [x] Step 5: Split message into words, emit `message_chunk` per word
  - [x] Step 6: Wait 50ms between each `message_chunk` (use asyncio.sleep)
  - [x] Step 7: Emit `done` event with session_id and metadata
  - [x] Use mock service to generate message content
  - [x] Write tests for event sequence and timing

- [x] Task 4: Reuse mock service logic (AC: 4, 5, 6)
  - [x] Import `generate_mock_response` from `src/services/mock_data.py`
  - [x] Use mock response to get message text, agent, logs
  - [x] Extract agent status updates from mock response
  - [x] Extract retrieval logs from mock response
  - [x] Ensure consistency with non-streaming `/api/chat` endpoint
  - [x] No need to modify mock_data.py (reuse as-is)

- [x] Task 5: Add configurable streaming speed (AC: 9)
  - [x] Add `STREAM_DELAY_MS` to `src/config.py` (default: 50)
  - [x] Use config value in `asyncio.sleep(STREAM_DELAY_MS / 1000)`
  - [x] Allow environment variable override: `STREAM_DELAY_MS=100`
  - [x] Document config option in `.env.template`
  - [x] Write test with different delay values

- [x] Task 6: Handle client disconnect gracefully (AC: 11)
  - [x] Wrap event generator in try/except for `asyncio.CancelledError`
  - [x] Log disconnect at INFO level (not ERROR)
  - [x] Clean up any resources on disconnect
  - [x] Verify no stack traces in logs on disconnect
  - [x] Write test that simulates client disconnect

- [x] Task 7: Add endpoint to API documentation (AC: 1)
  - [x] Add docstring to chat_stream function with full description
  - [x] Document request body (ChatRequest schema)
  - [x] Document response format (SSE event stream)
  - [x] List all event types and their data structures
  - [x] Verify endpoint appears in `/docs` (FastAPI auto-docs)

- [x] Task 8: Write comprehensive tests (AC: All)
  - [x] Test endpoint returns 200 with correct content-type
  - [x] Test event sequence (agent_status → logs → chunks → done)
  - [x] Test message chunking (verify word-by-word split)
  - [x] Test event format (valid SSE syntax)
  - [x] Test JSON parsing of event data
  - [x] Test client disconnect handling
  - [x] Test CORS headers present
  - [x] Verify no memory leaks (verify generator cleanup)

## Dev Notes

### Previous Story Context
From Story 2.2 (Backend Chat Endpoint):
- Existing `/api/chat` endpoint with ChatRequest/ChatResponse models
- Mock service (`generate_mock_response`) generates realistic responses
- Mock service includes agent routing, retrieval logs, metrics
- Enum synchronization with frontend (AgentId, AgentStatus, MessageRole)

From Story 3.1 (Extend ChatResponse):
- ChatResponse includes agent_status, metrics, retrieval_logs
- AgentMetrics model: tokens_used, cost_usd, latency_ms, cache_status
- LogEntry model: type, timestamp, data (polymorphic)

### Server-Sent Events (SSE) Specification
**[Source: W3C SSE Spec, Epic 4 spec]**

**Event Format:**
```
event: message_chunk
data: {"content":"Hello "}

event: message_chunk
data: {"content":"world"}

event: done
data: {"session_id":"550e8400-..."}

```

**Key Requirements:**
- Each event starts with `event: {type}\n`
- Data line: `data: {json_string}\n`
- Empty line `\n` marks end of event
- Data MUST be valid JSON string (serialize with json.dumps)
- Event names are case-sensitive

### FastAPI StreamingResponse
**[Source: FastAPI docs, Epic 4 spec]**

```python
from fastapi.responses import StreamingResponse
import asyncio

@router.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    async def event_generator():
        yield create_sse_event("agent_status", {
            "agent": "BILLING",
            "status": "ACTIVE"
        })
        await asyncio.sleep(0.05)  # 50ms delay

        yield create_sse_event("message_chunk", {
            "content": "Hello "
        })
        await asyncio.sleep(0.05)

        yield create_sse_event("done", {
            "session_id": str(request.session_id)
        })

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "X-Accel-Buffering": "no",  # Disable nginx buffering
            "Connection": "keep-alive",
        }
    )
```

### SSE Utility Function
**[Source: Epic 4 spec]**

```python
# src/services/sse_utils.py
import json
from typing import Any

def create_sse_event(event: str, data: dict[str, Any]) -> str:
    """
    Create a Server-Sent Event formatted string.

    Args:
        event: Event type (e.g., 'message_chunk', 'done')
        data: Event data (will be JSON serialized)

    Returns:
        SSE formatted string: "event: {type}\ndata: {json}\n\n"
    """
    return f"event: {event}\ndata: {json.dumps(data)}\n\n"
```

### Event Data Structures
**[Source: Epic 4 spec, Story 3.1]**

```python
# message_chunk event
{
    "content": str  # Partial message text
}

# agent_status event
{
    "agent": AgentId,  # "ORCHESTRATOR", "BILLING", etc.
    "status": AgentStatus  # "IDLE", "ROUTING", "ACTIVE", "COMPLETE"
}

# retrieval_log event
{
    "type": LogType,  # "query_analysis", "vector_search", "cache_operation"
    "timestamp": str,  # ISO format datetime
    "data": dict  # Polymorphic based on type
}

# done event
{
    "session_id": str,  # UUID
    "metadata": {
        "tokens_used": int,
        "cost_usd": float,
        "latency_ms": int,
        "cache_status": str
    }
}
```

### Message Chunking Strategy
**[Source: Epic 4 spec]**

```python
# Split message into words for streaming
message = "We offer three pricing tiers..."
words = message.split()  # ["We", "offer", "three", "pricing", "tiers..."]

for word in words:
    yield create_sse_event("message_chunk", {
        "content": word + " "  # Add space after each word
    })
    await asyncio.sleep(STREAM_DELAY_MS / 1000)
```

**Alternative:** Character-by-character streaming (slower, more dramatic effect)
```python
for char in message:
    yield create_sse_event("message_chunk", {
        "content": char
    })
    await asyncio.sleep(0.01)  # 10ms per character
```

Recommendation: Word-by-word for better readability and performance

### Event Generator Sequence
**[Source: Epic 4 spec]**

```python
async def event_generator():
    # 1. Generate mock response (reuse existing service)
    mock_response = generate_mock_response(request.message, request.session_id)

    # 2. Emit orchestrator routing
    yield create_sse_event("agent_status", {
        "agent": "ORCHESTRATOR",
        "status": "ROUTING"
    })
    await asyncio.sleep(0.1)  # 100ms

    # 3. Orchestrator idle, target agent active
    yield create_sse_event("agent_status", {
        "agent": "ORCHESTRATOR",
        "status": "IDLE"
    })
    yield create_sse_event("agent_status", {
        "agent": mock_response.agent,
        "status": "ACTIVE"
    })
    await asyncio.sleep(0.1)

    # 4. Emit retrieval logs
    for log in mock_response.retrieval_logs:
        yield create_sse_event("retrieval_log", log.model_dump())
        await asyncio.sleep(0.05)

    # 5. Stream message chunks
    for word in mock_response.message.content.split():
        yield create_sse_event("message_chunk", {
            "content": word + " "
        })
        await asyncio.sleep(STREAM_DELAY_MS / 1000)

    # 6. Final agent status
    yield create_sse_event("agent_status", {
        "agent": mock_response.agent,
        "status": "COMPLETE"
    })

    # 7. Done event
    yield create_sse_event("done", {
        "session_id": str(request.session_id),
        "metadata": mock_response.metrics.model_dump()
    })
```

### Configuration Management
**[Source: architecture/10-backend-architecture.md, src/config.py]**

```python
# src/config.py
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    # ... existing settings

    # SSE Streaming Configuration
    STREAM_DELAY_MS: int = 50  # Milliseconds between chunks

    class Config:
        env_file = ".env"
```

Update `.env.template`:
```bash
# SSE Streaming
STREAM_DELAY_MS=50  # Delay between message chunks (ms)
```

### CORS Configuration
**[Source: FastAPI CORS docs]**

```python
# src/main.py (if not already configured)
from fastapi.middleware.cors import CORSMiddleware

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],  # Frontend dev server
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
```

**For SSE specifically:**
- CORS headers required for EventSource API
- `Access-Control-Allow-Origin: http://localhost:3000`
- `Access-Control-Allow-Credentials: true`

### Client Disconnect Handling
**[Source: FastAPI docs, Python asyncio]**

```python
async def event_generator():
    try:
        # ... stream events
        yield create_sse_event("done", {...})
    except asyncio.CancelledError:
        # Client disconnected
        logger.info("Client disconnected from SSE stream")
        # Cleanup if needed
        raise  # Re-raise to properly close connection
```

### Error Handling
**[Source: Epic 4 spec]**

- Don't crash on client disconnect (CancelledError is normal)
- Log client disconnects at INFO level, not ERROR
- Gracefully handle exceptions in mock service
- If error occurs mid-stream, send error event before closing:

```python
try:
    # ... normal streaming
except Exception as e:
    logger.error(f"Error during SSE stream: {e}")
    yield create_sse_event("error", {
        "message": "Internal server error",
        "code": "STREAM_ERROR"
    })
```

### File Structure
**[Source: architecture/source-tree.md]**

```
orchestratai_api/src/
├── api/routes/
│   └── chat.py              # UPDATE: Add chat_stream endpoint
├── services/
│   ├── mock_data.py         # REUSE: No changes needed
│   └── sse_utils.py         # NEW: SSE formatting utility
├── config.py                # UPDATE: Add STREAM_DELAY_MS
└── models/
    ├── enums.py             # REUSE: No changes
    └── schemas.py           # REUSE: Reuse ChatRequest, ChatResponse
```

### Testing

**[Source: architecture/15-testing-strategy.md]**

**Test File Locations:**
- `orchestratai_api/tests/test_sse_utils.py` (NEW)
- `orchestratai_api/tests/test_chat_stream_endpoint.py` (NEW)

**Testing Framework:**
- pytest with pytest-asyncio
- httpx AsyncClient for endpoint testing
- Mock asyncio.sleep for faster tests

**Test Coverage Requirements:**
- Minimum 90% coverage required
- Test SSE event formatting
- Test event sequence and content
- Test client disconnect handling
- Test configuration (different delay values)

**Example Test Structure:**
```python
import pytest
from httpx import AsyncClient
from src.main import app

@pytest.mark.asyncio
async def test_chat_stream_endpoint():
    async with AsyncClient(app=app, base_url="http://test") as client:
        response = await client.post(
            "/api/chat/stream",
            json={
                "message": "What are your pricing tiers?",
                "session_id": "550e8400-e29b-41d4-a716-446655440000"
            }
        )

    assert response.status_code == 200
    assert response.headers["content-type"] == "text/event-stream"

    # Parse SSE events
    events = parse_sse_events(response.text)

    # Verify event sequence
    assert events[0]["event"] == "agent_status"
    assert events[0]["data"]["status"] == "ROUTING"

    assert events[-1]["event"] == "done"
    assert "session_id" in events[-1]["data"]

def parse_sse_events(text: str) -> list[dict]:
    """Parse SSE stream into list of events"""
    events = []
    current_event = {}

    for line in text.split('\n'):
        if line.startswith('event:'):
            current_event['event'] = line.split(':', 1)[1].strip()
        elif line.startswith('data:'):
            import json
            current_event['data'] = json.loads(line.split(':', 1)[1].strip())
        elif line == '':
            if current_event:
                events.append(current_event)
                current_event = {}

    return events
```

### Performance Considerations
**[Source: Epic 4 spec]**

- Use `asyncio.sleep()` for non-blocking delays
- Don't hold database connections during stream (not applicable for MVP)
- Generator pattern ensures memory efficiency (yields, not builds list)
- Connection timeout: Default 60s, extend if needed for long responses

### Nginx Buffering
**[Source: SSE best practices]**

- Header `X-Accel-Buffering: no` disables nginx buffering
- Critical for SSE to work behind nginx reverse proxy
- Without this, events are buffered and sent in batches
- Railway.app and other platforms may use nginx, so include this header

### Testing with curl
**[Source: Epic 4 spec]**

Test endpoint manually:
```bash
curl -N -X POST http://localhost:8000/api/chat/stream \
  -H "Content-Type: application/json" \
  -d '{
    "message": "What are your pricing tiers?",
    "session_id": "550e8400-e29b-41d4-a716-446655440000"
  }'
```

Expected output (streaming):
```
event: agent_status
data: {"agent":"ORCHESTRATOR","status":"ROUTING"}

event: message_chunk
data: {"content":"We "}

event: message_chunk
data: {"content":"offer "}

event: done
data: {"session_id":"550e8400-...","metadata":{...}}
```

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-11-01 | 1.0 | Initial story creation | Bob (Scrum Master) |
| 2025-11-01 | 2.0 | Story implementation completed - SSE streaming endpoint fully functional with 98.47% test coverage | James (Dev Agent) |

## Dev Agent Record

### Agent Model Used
Claude Sonnet 4.5 (claude-sonnet-4-5-20250929)

### Debug Log References
No significant debugging issues encountered. All tests passed on first run after fixing AsyncClient usage pattern.

### Completion Notes List
- Successfully implemented SSE streaming endpoint at `/api/chat/stream`
- All 8 tasks completed with full test coverage (98.47% overall project coverage)
- 172 total tests passing (13 new SSE utils tests + 17 new chat stream endpoint tests)
- Linting passes with zero errors
- Event streaming follows W3C SSE specification precisely
- Client disconnect handling implemented with graceful error logging
- Configurable streaming speed via STREAM_DELAY_MS environment variable
- Full API documentation in endpoint docstrings (auto-appears in FastAPI /docs)
- Reused existing mock_data.py service without modifications (maintained consistency)

### File List
**New Files:**
- `orchestratai_api/src/services/sse_utils.py` - SSE event formatting utility
- `orchestratai_api/tests/test_sse_utils.py` - Unit tests for SSE utils (13 tests)
- `orchestratai_api/tests/test_chat_stream_endpoint.py` - Integration tests for streaming endpoint (17 tests)

**Modified Files:**
- `orchestratai_api/src/api/routes/chat.py` - Added chat_stream endpoint
- `orchestratai_api/src/config.py` - Added STREAM_DELAY_MS configuration
- `orchestratai_api/.env.template` - Documented STREAM_DELAY_MS variable

## QA Results
<!-- To be filled by QA Agent -->
