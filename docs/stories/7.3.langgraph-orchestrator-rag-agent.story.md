# <!-- Powered by BMAD™ Core -->

## Status
Draft

## Story
**As a** backend developer,
**I want** a LangGraph-based orchestrator with guide/delegate modes and a working RAG agent,
**so that** the system can intelligently route queries and retrieve relevant documents for contextual responses

## Acceptance Criteria
1. Meta questions such as "What can you help with?" return orchestrator-only responses (guide mode) with accurate metrics and SSE events
2. Domain questions trigger the RAG agent, yielding retrieval logs, metrics, and documents from Chroma
3. `/api/chat` returns the exact schema consumed by the frontend today
4. SSE stream emits `agent_status`, `retrieval_log`, `message_chunk`, and `done` in the order expected by `useStreaming`
5. Tests in `tests/orchestrator` run green with ≥90% coverage
6. Manual run (`uv run python -m pytest tests/orchestrator`) plus smoke test via `uv run uvicorn src.main:app --reload` confirms the UI renders live data

## Tasks / Subtasks

- [ ] Task 1: Agent instrumentation (AC: 2)
  - [ ] Create `orchestratai_api/src/agents/__init__.py`
  - [ ] Create `orchestratai_api/src/agents/base.py` with `BaseAgent` ABC
  - [ ] Add `__init__(role: AgentRole, provider: BaseLLMProvider)` constructor
  - [ ] Define abstract method `async def run(request: ChatRequest, **kwargs) -> ChatResponse`
  - [ ] Implement `async def _record(start: float, result: LLMCallResult, extra: dict) -> AgentMetrics`
  - [ ] Capture: agent_id, provider, model, tokens_input, tokens_output, cost_total, latency_ms
  - [ ] Use `time.perf_counter()` for latency measurement
  - [ ] Map `LLMCallResult` to `AgentMetrics` schema from existing models

- [ ] Task 2: RAG worker (AC: 2)
  - [ ] Create `orchestratai_api/src/agents/workers/__init__.py`
  - [ ] Create `orchestratai_api/src/agents/workers/rag_agent.py`
  - [ ] Extend `BaseAgent`
  - [ ] Inject `ChromaVectorStore` and embeddings provider via `ProviderFactory`
  - [ ] Implement retrieval workflow:
    - Embed query using embeddings provider
    - Call `ChromaVectorStore.similarity_search(query, k=5)`
    - Assemble context from retrieved documents
    - Call provider with system prompt + context + query
  - [ ] Return `ChatResponse` with message + retrieval logs
  - [ ] Return `AgentMetrics` with sources_used, retrieval_latency
  - [ ] Log retrieved documents to `retrieval_logs` (type: VECTOR_SEARCH)
  - [ ] Include similarity scores in logs

- [ ] Task 3: LangGraph state + nodes (AC: 1, 2, 4)
  - [ ] Create `orchestratai_api/src/agents/orchestrator.py`
  - [ ] Import `StateGraph` from `langgraph.graph`
  - [ ] Define `OrchestratorState` TypedDict:
    - `messages: list[dict[str, str]]`
    - `analysis: dict[str, Any]`  # intent, confidence, routing decision
    - `route: str`  # "guide" or "delegate"
    - `result: ChatResponse | None`
    - `session_id: str`
  - [ ] Create `analyse_query` node function
    - Resolve orchestrator analysis provider via `ProviderFactory` (Claude 3.5 Sonnet)
    - Classify intent: META_QUESTION, DOMAIN_QUESTION, POLICY_QUESTION, etc.
    - Return confidence score
    - Update state with analysis
  - [ ] Create `decide_route` conditional edge function
    - If intent is META_QUESTION → return "guide"
    - Otherwise → return "delegate"
  - [ ] Create `guide_user` node function
    - Resolve orchestrator guide provider via `ProviderFactory` (Claude 3 Haiku)
    - Generate direct response without worker agents
    - Update state with ChatResponse
  - [ ] Create `delegate_to_worker` node function
    - Route to appropriate worker based on analysis (DOMAIN_QUESTION → RAG)
    - Fetch worker instance from dependency container / ProviderFactory
    - Invoke worker: `await rag_agent.run(request, session_id=session_id)`
    - Update state with worker ChatResponse
  - [ ] Build StateGraph workflow
  - [ ] Set entry point: "analyse"
  - [ ] Add conditional edges from "analyse" to "guide" or "delegate"
  - [ ] Add edges from "guide" and "delegate" to END
  - [ ] Compile graph: `orchestrator = workflow.compile()`

- [ ] Task 4: Agent service bridge (AC: 3, 4)
  - [ ] Create `orchestratai_api/src/services/agent_service.py`
  - [ ] Implement `class AgentService`
  - [ ] Add method `async def process_chat(request: ChatRequest) -> ChatResponse`
  - [ ] Translate `ChatRequest` to `OrchestratorState`
  - [ ] Include session_id, message history
  - [ ] Call `orchestrator.ainvoke(state)` (non-streaming)
  - [ ] Extract final `ChatResponse` from state
  - [ ] Add method `async def process_chat_stream(request: ChatRequest) -> AsyncIterator[str]`
  - [ ] Call `orchestrator.astream_events(state, version="v1")`
  - [ ] Translate LangGraph events (`event["type"]`) to SSE events:
    - Model start → `agent_status` event
    - Retriever start → `retrieval_log` event
    - Model stream chunks → `message_chunk` events
    - Workflow end → `done` event
  - [ ] Format SSE: `data: {json}\n\n`
  - [ ] Preserve existing event schema (no frontend changes)

- [ ] Task 5: FastAPI endpoints (AC: 3, 4)
  - [ ] Update `orchestratai_api/src/api/routes/chat.py`
  - [ ] Inject `AgentService` dependency
  - [ ] Update `POST /api/chat` endpoint:
    - Remove mock_data import
    - Call `agent_service.process_chat(request)`
    - Return ChatResponse
    - Keep existing response schema
  - [ ] Update `POST /api/chat/stream` endpoint:
    - Remove mock_data import
    - Call `agent_service.process_chat_stream(request)`
    - Yield SSE events
    - Keep existing headers (CORS, Cache-Control, Content-Type: text/event-stream)
  - [ ] Ensure CORS, error handling remain unchanged

- [ ] Task 6: Testing - Unit tests (AC: 5)
  - [ ] Create `orchestratai_api/tests/orchestrator/__init__.py`
  - [ ] Create `orchestratai_api/tests/orchestrator/test_decision_logic.py`
  - [ ] Test `decide_route` with various intents
  - [ ] Test META_QUESTION → "guide"
  - [ ] Test DOMAIN_QUESTION → "delegate"
  - [ ] Create `orchestratai_api/tests/orchestrator/test_analyse_query.py`
  - [ ] Mock orchestrator analysis provider
  - [ ] Test intent classification
  - [ ] Test confidence scoring
  - [ ] Create `orchestratai_api/tests/orchestrator/test_rag_agent.py`
  - [ ] Mock ChromaVectorStore responses
  - [ ] Test retrieval workflow
  - [ ] Test document context assembly
  - [ ] Test metrics recording
  - [ ] Verify retrieval_logs populated

- [ ] Task 7: Testing - Integration tests (AC: 5, 6)
  - [ ] Create `orchestratai_api/tests/orchestrator/test_orchestrator_integration.py`
  - [ ] Use temporary ChromaVectorStore
  - [ ] Seed with test documents
  - [ ] Test end-to-end guide mode:
    - Send meta question
    - Verify orchestrator-only response
    - Verify no worker invoked
    - Verify agent_status events
  - [ ] Test end-to-end delegate mode (RAG):
    - Send domain question
    - Verify RAG agent invoked
    - Verify documents retrieved
    - Verify retrieval_logs populated
    - Verify metrics accurate
  - [ ] Test SSE streaming:
    - Verify event order: agent_status → retrieval_log → message_chunk → done
    - Verify event schema matches frontend expectations
  - [ ] Clean up temporary ChromaDB after tests
  - [ ] Achieve ≥90% coverage

- [ ] Task 8: Manual smoke testing (AC: 6)
  - [ ] Start backend: `uv run uvicorn orchestratai_api.src.main:app --reload`
  - [ ] Verify ChromaDB container running: `docker compose up chromadb`
  - [ ] Test meta question via curl or Postman:
    - POST /api/chat: `{"message": "What can you help with?"}`
    - Verify guide mode response
    - Verify agent_status shows orchestrator only
  - [ ] Test domain question:
    - POST /api/chat: `{"message": "What is RAG?"}`
    - Verify RAG agent invoked
    - Verify retrieval_logs show documents
    - Verify metrics populated
  - [ ] Test streaming endpoint:
    - GET /api/chat/stream
    - Verify SSE events stream correctly
  - [ ] Open frontend: `npm run dev`
  - [ ] Verify UI panels render real data
  - [ ] Verify Agent Panel shows orchestrator + RAG
  - [ ] Verify Retrieval Panel shows retrieved documents
  - [ ] Verify Metrics Panel shows real tokens/cost/latency
  - [ ] Document any issues in debug log

## Dev Notes

### Architecture Context
**[Source: docs/agent-planning/agent-feature-planning.md, Story 7.3]**

Replace the mock response pipeline with a LangGraph workflow mirroring the multi-agent supervisor pattern. This story delivers an orchestrator with guide/delegate modes plus the first real worker (RAG). The orchestrator will:

1. Analyse every request with Claude 3.5 Sonnet (analysis mode)
2. Decide between `guide` (answer directly) or `delegate` (route to RAG)
3. Stream events via SSE so the existing frontend panels light up with real data

### System Architecture Diagram
**[Source: agent-feature-planning.md:92-177]**

```
┌──────────────────────────────────────────────────────────────────────┐
│                         USER INTERFACE                               │
│                    (Next.js 15 + React 19)                           │
│                     Already Built - No Changes                       │
└──────────────────────┬───────────────────────────────────────────────┘
                       │ HTTP/SSE
                       ▼
┌──────────────────────────────────────────────────────────────────────┐
│                      FASTAPI BACKEND                                 │
│                    /api/chat endpoint                                │
│                         │                                            │
│                         ▼                                            │
│              ┌─────────────────────┐                                 │
│              │  AGENT SERVICE      │  ← NEW: Bridge Layer            │
│              │  (agent_service.py) │    Translates HTTP ↔ LangGraph  │
│              └──────────┬──────────┘                                 │
│                         │                                            │
│                         ▼                                            │
│           ┌─────────────────────────────┐                            │
│           │   LANGGRAPH ORCHESTRATOR    │  ← NEW: Orchestration      │
│           │   (Bedrock Claude 3.5)      │                            │
│           │                             │                            │
│           │  ┌────────────────────┐     │                            │
│           │  │ 1. Analyze Query   │─────┼── META_ROUTING?            │
│           │  └────────────────────┘     │      │                     │
│           │           │                 │      ├─Yes─→ GUIDE MODE    │
│           │           ▼                 │      │      (Answer        │
│           │  ┌────────────────────┐     │      │       directly)     │
│           │  │ 2. Route Decision  │─────┼──────┘                     │
│           │  └────────────────────┘     │      └─No──→ DELEGATE      │
│           │           │                 │              MODE          │
│           └───────────┼─────────────────┘                            │
│                       │                                              │
│        ┌──────────────┼──────────────────────────────────────────┐   │
│        │              │                                          │   │
│        ▼              ▼                                          ▼   │
│   ┌────────┐    ┌─────────┐                                          │
│   │  RAG   │    │ Direct  │  (CAG + Hybrid in future stories)        │
│   │ AGENT  │    │ Orchestrator │                                     │
│   │        │    │ Response│                                          │
│   │ OpenAI │    │ (Haiku) │                                          │
│   │ GPT-4  │    │         │                                          │
│   │ Turbo  │    │         │                                          │
│   └────┬───┘    └─────────┘                                          │
│        │                                                             │
└────────┼─────────────────────────────────────────────────────────────┘
         │
         ▼
┌─────────────────────────────────────────────────────────────────────┐
│                        DATA LAYER                                   │
│  ┌──────────────┐                                                    │
│  │ VECTOR DB    │                                                    │
│  │ (ChromaDB)   │                                                    │
│  │              │                                                    │
│  │ • Embeddings │                                                    │
│  │ • Similarity │                                                    │
│  │ • Retrieval  │                                                    │
│  └──────────────┘                                                    │
└─────────────────────────────────────────────────────────────────────┘
```

### Data Flow Example
**[Source: agent-feature-planning.md:179-209]**

**User Query: "What is RAG?"**

```
1. User types in chat → Frontend (React)
2. Frontend → POST /api/chat → FastAPI
3. FastAPI → AgentService.process_chat()
4. AgentService → LangGraph Orchestrator.ainvoke()
5. Orchestrator → Analyze: "DOMAIN_QUESTION" → DELEGATE MODE
6. Orchestrator → Route to RAG Agent (requires retrieval)
7. RAG Agent → Embed query (OpenAI text-embedding-3-large)
8. RAG Agent → ChromaDB.similarity_search(top_k=5)
9. RAG Agent → GPT-4 Turbo.generate(context + query)
10. RAG Agent → Return ChatResponse + metrics
11. AgentService → Convert LangGraph output to ChatResponse
12. FastAPI → Stream events via SSE to Frontend
13. Frontend → Existing panels display real data (no code changes)
```

**Metrics Automatically Returned:**
- Orchestrator: 200 tokens, $0.0006, 800ms (Bedrock Sonnet)
- RAG Agent: 3,500 tokens, $0.0405, 2,800ms (OpenAI GPT-4)
- **Total: $0.0411, 3,600ms, 5 documents retrieved**

### BaseAgent Implementation
**[Source: agent-feature-planning.md:726-749]**

```python
class BaseAgent(ABC):
    def __init__(self, *, role: AgentRole, provider: BaseLLMProvider):
        self.role = role
        self.provider = provider

    async def _record(self, *, start: float, result: LLMCallResult, extra: dict | None = None) -> AgentMetrics:
        return AgentMetrics(
            agent_id=self.role.value,
            provider=self.provider.__class__.__name__.lower(),
            model=result.model,
            tokens_input=result.tokens_input,
            tokens_output=result.tokens_output,
            cost_total=result.cost,
            latency_ms=(time.perf_counter() - start) * 1000,
            extra=extra or {},
        )
```

Expose `async def run(self, request: ChatRequest, **kwargs) -> ChatResponse` to standardise worker signatures.

### RAG Agent Implementation
**[Source: agent-feature-planning.md:749-753]**

Implement the retrieval workflow:
1. Embed query → `ChromaVectorStore.similarity_search`
2. Assemble context
3. Call provider

Return both `ChatResponse` (message + logs) and `AgentMetrics` from `_record`.

### LangGraph State + Nodes
**[Source: agent-feature-planning.md:755-777]**

```python
from langgraph.graph import StateGraph, END

workflow = StateGraph(OrchestratorState)
workflow.add_node("analyse", analyse_query)
workflow.add_node("guide", guide_user)
workflow.add_node("delegate", delegate_to_worker)

workflow.set_entry_point("analyse")
workflow.add_conditional_edges(
    "analyse",
    decide_route,
    {"guide": "guide", "delegate": "delegate"},
)
workflow.add_edge("guide", END)
workflow.add_edge("delegate", END)
orchestrator = workflow.compile()
```

- `analyse_query`: Call orchestrator analysis provider (Claude Sonnet), return intent/confidence
- `guide_user`: Use orchestrator guide provider (Claude Haiku) for direct response
- `delegate_to_worker`: Invoke RAG agent, store result in state

### Agent Service Bridge
**[Source: agent-feature-planning.md:779-785]**

Translate incoming `ChatRequest` → `OrchestratorState` (include history + session).

Call `orchestrator.astream_events` to surface SSE events mirroring existing mock event types:
- `agent_status`
- `retrieval_log`
- `message_chunk`
- `done`

Convert final state back to existing `ChatResponse` schema.

### SSE Event Mapping
**[Source: agent-feature-planning.md:779-785]**

Map LangGraph events to frontend SSE expectations:

| LangGraph Event | SSE Event Type | Payload |
|-----------------|----------------|---------|
| `on_chat_model_start` | `agent_status` | Agent name, status: "processing" |
| `on_retriever_start` | `retrieval_log` | Log type: VECTOR_SEARCH, documents |
| `on_chat_model_stream` | `message_chunk` | Chunk of response text |
| `on_chain_end` | `done` | Final ChatResponse with all metrics |

### Existing ChatResponse Schema
**[Source: orchestratai_api/src/models/chat.py]**

Keep the exact same schema - frontend expects:

```python
class ChatResponse:
    id: str
    message: str
    agents: list[AgentStatus]  # Orchestrator + workers
    agent_metrics: AgentMetrics
    retrieval_logs: list[RetrievalLog]
    execution_graph: ExecutionGraph
    metadata: dict
```

No changes to this model - just populate with real data instead of mocks.

### File Structure
**[Source: architecture/source-tree.md]**

```
orchestratai_api/src/
├── agents/
│   ├── __init__.py
│   ├── base.py                # NEW: BaseAgent ABC
│   ├── orchestrator.py        # NEW: LangGraph orchestrator
│   └── workers/
│       ├── __init__.py
│       └── rag_agent.py       # NEW: RAG worker agent
├── services/
│   └── agent_service.py       # NEW: HTTP ↔ LangGraph bridge
└── routers/
    └── chat.py                # UPDATE: Use AgentService

orchestratai_api/tests/orchestrator/
├── __init__.py
├── test_decision_logic.py     # NEW: Routing tests
├── test_analyse_query.py      # NEW: Intent classification tests
├── test_rag_agent.py          # NEW: RAG workflow tests
└── test_orchestrator_integration.py  # NEW: End-to-end tests
```

### Testing

**[Source: architecture/15-testing-strategy.md]**

**Test File Locations:**
- `orchestratai_api/tests/orchestrator/test_decision_logic.py`
- `orchestratai_api/tests/orchestrator/test_analyse_query.py`
- `orchestratai_api/tests/orchestrator/test_rag_agent.py`
- `orchestratai_api/tests/orchestrator/test_orchestrator_integration.py`

**Testing Framework:**
- pytest + pytest-asyncio for async tests
- Mock provider responses to assert state updates
- Temporary ChromaDB for integration tests
- Mock LangGraph events for SSE testing

**Test Coverage Requirements:**
- Minimum 90% coverage required
- Unit-test decision logic (decide_route)
- Mock provider responses
- Integration test RAG path end-to-end with temp Chroma

**Example Test Structure:**
```python
import pytest

@pytest.mark.asyncio
async def test_decide_route_meta_question(orchestrator):
    state = await orchestrator.ainvoke({"messages": [{"role": "user", "content": "What can you do?"}]})
    assert state["route"] == "guide"
    assert "orchestrator" in state["result"].agent_status
```

### Environment Variables
**[Source: agent-feature-planning.md:367-377]**

```bash
# Orchestrator uses TWO models:
ORCHESTRATOR_ANALYSIS_MODEL=anthropic.claude-3-5-sonnet-20241022-v2:0  # Strategic routing
ORCHESTRATOR_GUIDE_MODEL=anthropic.claude-3-haiku-20240307-v1:0        # Fast guide mode

# Worker agent models:
DEFAULT_RAG_MODEL=gpt-4-turbo
```

### Python Dependencies
**[Source: agent-feature-planning.md:388-418]**

Ensure these are in `pyproject.toml`:

```toml
dependencies = [
    # Orchestration (from Story 7.1)
    "langgraph>=0.0.20",
    "langchain-core>=0.1.0",

    # Already added in previous stories
    "openai>=1.12.0",
    "boto3>=1.34.0",
    "chromadb>=0.4.22",
]
```

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-11-02 | 1.0 | Initial story creation | Bob (Scrum Master) |

## Dev Agent Record

### Agent Model Used
(To be filled by dev agent)

### Debug Log References
(To be filled by dev agent)

### Completion Notes List
(To be filled by dev agent)

### File List
(To be filled by dev agent)

## QA Results
(To be filled by QA agent)
