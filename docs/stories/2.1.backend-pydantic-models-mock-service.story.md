# Story 2.1: Backend Pydantic Models and Mock Data Service

## Status
Draft

## Story
**As a** backend developer,
**I want** Pydantic models for chat request/response and a mock data service that generates realistic agent responses,
**so that** the frontend can integrate with a working API endpoint that simulates multi-agent routing behavior.

## Acceptance Criteria
1. Pydantic schemas created for `ChatRequest` and `ChatResponse` matching the OpenAPI specification
2. Python enums mirror TypeScript enums exactly (AgentId, MessageRole, LogType, LogStatus)
3. Mock data service implements keyword-based routing logic (billing, technical, policy, orchestrator)
4. Mock responses include all required fields: message, agent, confidence, logs, metrics
5. Mock service returns realistic data with varying latency (800ms-2000ms simulation range)
6. All Pydantic models have proper field validation (min/max lengths, value ranges)
7. Backend passes enum validation script (Python enums match TypeScript)

## Tasks / Subtasks

- [ ] Create Python enums in `orchestratai_api/src/models/enums.py` (AC: 2, 7)
  - [ ] Define `AgentId` enum with values: orchestrator, billing, technical, policy
  - [ ] Define `MessageRole` enum with values: user, assistant, system
  - [ ] Define `LogType` enum with values: routing, vector_search, cache, documents
  - [ ] Define `LogStatus` enum with values: success, warning, error
  - [ ] Run enum validation script to ensure sync with TypeScript enums

- [ ] Create Pydantic schemas in `orchestratai_api/src/models/schemas.py` (AC: 1, 6)
  - [ ] Define `ChatRequest` model with fields: message (str, 1-2000 chars), session_id (UUID)
  - [ ] Define `ChatMetrics` model with fields: tokensUsed (int), cost (float), latency (int)
  - [ ] Define `RetrievalLog` model with fields: id, type, title, data, timestamp, status
  - [ ] Define `ChatResponse` model with fields: message, agent, confidence (0.0-1.0), logs, metrics
  - [ ] Add field validators for confidence range and message length
  - [ ] Add example values in Field() for OpenAPI documentation

- [ ] Create mock data service in `orchestratai_api/src/services/mock_data.py` (AC: 3, 4, 5)
  - [ ] Implement `route_message(message: str) -> AgentId` function with keyword matching
  - [ ] Create response templates dict with 3-5 templates per agent type
  - [ ] Implement `generate_mock_response(message: str) -> ChatResponse` function
  - [ ] Add random latency simulation (800-2000ms) metadata to response
  - [ ] Generate realistic metrics: tokens (200-800), cost ($0.001-0.005), latency (800-2000ms)
  - [ ] Generate mock retrieval logs (2-4 logs per response) with varying types
  - [ ] Randomize confidence scores (0.85-0.98 for matched agents, 0.70-0.85 for orchestrator)

- [ ] Write unit tests for mock data service (AC: 3, 4)
  - [ ] Test billing keyword routing ("price", "billing", "cost", "subscription")
  - [ ] Test technical keyword routing ("error", "bug", "technical", "api")
  - [ ] Test policy keyword routing ("policy", "refund", "terms", "cancel")
  - [ ] Test orchestrator fallback for non-matching messages
  - [ ] Test all required fields are present in ChatResponse
  - [ ] Test field validation (confidence range, message length)

## Dev Notes

### Previous Story Insights
- Epic 1 completed the TypeScript enums in `orchestratai_client/src/lib/enums.ts`
- Zod schemas exist in `orchestratai_client/src/lib/schemas.ts`
- Enum validation script exists at `scripts/validate-enums.ts` and runs in pre-commit hook
- Design token system is in place but not relevant for backend work

### Data Models
[Source: docs/architecture/4-data-models.md#42-message, #46-chatmetrics]

**ChatRequest:**
- `message`: string, 1-2000 characters
- `session_id`: UUID string format

**ChatResponse:**
- `message`: string (AI response text)
- `agent`: AgentId enum value
- `confidence`: number, 0.0 to 1.0 range
- `logs`: array of RetrievalLog objects
- `metrics`: ChatMetrics object

**ChatMetrics:**
- `tokensUsed`: integer (number of tokens consumed)
- `cost`: float (USD cost)
- `latency`: integer (milliseconds)

**RetrievalLog:**
- `id`: string UUID
- `type`: LogType enum (routing, vector_search, cache, documents)
- `title`: string
- `data`: dict (arbitrary JSON)
- `timestamp`: ISO 8601 string
- `status`: LogStatus enum (success, warning, error)

### API Specifications
[Source: docs/architecture/5-api-specification.md#rest-api-specification]

**POST /api/chat:**
- Request body: ChatRequest schema
- Response: ChatResponse schema (200) or ErrorResponse (400)
- Content-Type: application/json
- Validates with Pydantic models

**Enums (must match TypeScript exactly):**
- AgentId: orchestrator, billing, technical, policy
- MessageRole: user, assistant, system
- LogType: routing, vector_search, cache, documents
- LogStatus: success, warning, error

### File Locations
[Source: docs/architecture/10-backend-architecture.md#101-service-organization, docs/architecture/11-unified-project-structure.md]

**Create these files:**
```
orchestratai_api/src/
├── models/
│   ├── enums.py              # Python enums (mirror TypeScript)
│   └── schemas.py            # Pydantic request/response models
└── services/
    └── mock_data.py          # Mock response generator
```

**Test files:**
```
orchestratai_api/tests/
└── test_mock_data.py         # Unit tests for mock service
```

### Technical Constraints
[Source: docs/architecture/3-tech-stack.md]

- Python: 3.12
- FastAPI: 0.115+
- Pydantic: v2 (comes with FastAPI)
- Package manager: uv (use `uv add` not `pip install`)
- Testing: pytest 8.0+

### Mock Routing Logic
[Source: docs/stories/epic-2-chat-interface.md#lines-232-244]

Implement simple keyword matching:
```python
def route_message(message: str) -> AgentId:
    message_lower = message.lower()

    if any(word in message_lower for word in ["price", "billing", "cost", "subscription"]):
        return AgentId.BILLING
    elif any(word in message_lower for word in ["error", "bug", "technical", "api"]):
        return AgentId.TECHNICAL
    elif any(word in message_lower for word in ["policy", "refund", "terms", "cancel"]):
        return AgentId.POLICY
    else:
        return AgentId.ORCHESTRATOR
```

### Response Templates Example
Create dictionary with 3-5 response templates per agent:
```python
BILLING_RESPONSES = [
    "We offer three pricing tiers: Starter ($29/mo), Professional ($99/mo), and Enterprise (custom). Which would you like to learn more about?",
    "Your current subscription is the Professional plan at $99/month. Would you like to upgrade to Enterprise for advanced features?",
    # ... more templates
]
```

Randomly select template based on routed agent type.

### Testing

[Source: docs/architecture/15-testing-strategy.md#152-test-examples]

**Test Framework:** pytest 8.0+
**Test Location:** `orchestratai_api/tests/test_mock_data.py`
**Test Pattern:** Unit tests (70% of test pyramid)

**Test Structure:**
```python
async def test_chat_endpoint():
    # Arrange
    request_data = {...}

    # Act
    response = generate_mock_response(request_data.message)

    # Assert
    assert response.agent == AgentId.BILLING
    assert 0.0 <= response.confidence <= 1.0
```

**Required Test Cases:**
1. Test each keyword routing path (billing, technical, policy, orchestrator)
2. Test response includes all required fields
3. Test field validation catches invalid data
4. Test confidence scores are within valid range (0.0-1.0)

**Enum Validation:**
Run `bun run validate:enums` before committing (Husky pre-commit hook)

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-26 | 1.0 | Initial story creation | Scrum Master (Bob) |

## Dev Agent Record

### Agent Model Used
<!-- Populated by dev agent -->

### Debug Log References
<!-- Populated by dev agent -->

### Completion Notes List
<!-- Populated by dev agent -->

### File List
<!-- Populated by dev agent -->

## QA Results
<!-- Populated by QA agent -->
