# <!-- Powered by BMAD™ Core -->

## Status
Draft

## Story
**As a** QA engineer and backend developer,
**I want** comprehensive end-to-end testing with performance optimization and cost validation,
**so that** the multi-agent system is production-ready with predictable performance and costs

## Acceptance Criteria
1. E2E tests cover all user journeys (90%+ coverage)
2. p95 latency < 5 seconds for all agent types
3. Average cost per query ≤ $0.015
4. Cache hit rate ≥ 60% over diverse queries
5. System stable under 100 concurrent users
6. Fallback chain works for all failure scenarios
7. Documentation complete and accurate
8. All tests pass (unit, integration, E2E)

## Tasks / Subtasks

- [ ] Task 1: Write E2E integration tests (AC: 1)
  - [ ] Create `tests/e2e/__init__.py`
  - [ ] Create `tests/e2e/test_user_journey_rag.py`
  - [ ] Test complete RAG journey:
    - Start with empty session
    - Send: "What is RAG?"
    - Verify RAG agent invoked
    - Verify 5 documents retrieved
    - Verify response quality
    - Verify metrics accurate
  - [ ] Create `tests/e2e/test_user_journey_cag.py`
  - [ ] Test CAG journey:
    - First query: "Can I get a refund?"
    - Verify cache miss, Haiku called
    - Second identical query
    - Verify cache hit, cost = $0
  - [ ] Create `tests/e2e/test_user_journey_guide_mode.py`
  - [ ] Test guide mode:
    - Send: "What can you help with?"
    - Verify orchestrator-only response
    - Verify no worker agents called
  - [ ] Create `tests/e2e/test_user_journey_hybrid.py`
  - [ ] Test hybrid journey:
    - Send complex query
    - Verify both RAG and cache used
    - Verify contexts merged
  - [ ] Create `tests/e2e/test_concurrent_queries.py`
  - [ ] Test 10+ simultaneous queries (using `pytest-xdist -n 10` or async gather)
  - [ ] Verify all complete successfully
  - [ ] Verify no race conditions
  - [ ] Achieve 90%+ E2E coverage

- [ ] Task 2: Performance testing (AC: 2)
  - [ ] Create `tests/performance/__init__.py`
  - [ ] Create `tests/performance/test_latency_targets.py`
  - [ ] Measure latency for each agent type:
    - Orchestrator (guide mode): Target < 1s
    - RAG agent: Target < 5s
    - CAG agent (miss): Target < 1s
    - CAG agent (hit): Target < 500ms
    - Hybrid agent: Target < 5s
    - Direct agent: Target < 1s
  - [ ] Use percentiles: p50, p95, p99
  - [ ] Run 100 queries per agent type
  - [ ] Record results in CSV/JSON
  - [ ] Identify bottlenecks:
    - Vector search time
    - LLM generation time
    - Cache lookup time
    - Network overhead
  - [ ] Create performance report
  - [ ] Optimize slow operations:
    - If vector search > 2s: Adjust chunk size, reduce top-k
    - If LLM generation > 3s: Reduce max_tokens
    - If cache lookup > 100ms: Optimize Redis queries
  - [ ] Re-test after optimizations
  - [ ] Ensure p95 < 5s for all agents

- [ ] Task 3: Cost validation (AC: 3)
  - [ ] Create `tests/performance/test_cost_validation.py`
  - [ ] Run 100 diverse queries:
    - 70% policy/repeated (CAG territory)
    - 20% domain knowledge (RAG territory)
    - 7% complex (Hybrid territory)
    - 3% simple chat (Direct territory)
  - [ ] Measure actual cost per query type:
    - CAG hit: $0.00
    - CAG miss: ~$0.0004
    - RAG: ~$0.04
    - Hybrid: ~$0.02
    - Direct: ~$0.0007
  - [ ] Calculate weighted average
  - [ ] Expected: ~$0.010-0.015/query (70% cache hit rate)
  - [ ] If average > $0.015: Optimize
    - Increase cache TTL
    - Lower similarity threshold (more cache hits)
    - Reduce max_tokens for generation
  - [ ] Create cost breakdown report:
    - Cost by agent type
    - Cost by provider (OpenAI vs Bedrock)
    - Embeddings cost
    - Total monthly projection
  - [ ] Document actual cost distribution

- [ ] Task 4: Load testing (AC: 5)
  - [ ] Create `tests/performance/test_load.py`
  - [ ] Use `pytest-xdist` (for concurrent pytest) or `locust` (for sustained load) for load generation
  - [ ] Simulate 100 concurrent users
  - [ ] Ramp up: 0 → 100 users over 2 minutes
  - [ ] Sustained load: 100 users for 5 minutes
  - [ ] Ramp down: 100 → 0 users over 1 minute
  - [ ] Measure:
    - Requests per second
    - Error rate
    - Response time (p50, p95, p99)
    - ChromaDB performance
    - Redis performance
    - CPU/memory usage
  - [ ] Test ChromaDB under load:
    - Query latency during concurrent searches
    - Connection pool exhaustion
  - [ ] Test Redis under load:
    - Cache hit rate
    - Connection errors
  - [ ] Ensure system stable:
    - Error rate < 1%
    - No connection failures
    - No memory leaks
    - Response times within targets
  - [ ] Create load test report

- [ ] Task 5: Cache optimization (AC: 4)
  - [ ] Create `tests/performance/test_cache_optimization.py`
  - [ ] Test similarity threshold tuning:
    - Test 0.80, 0.85, 0.90, 0.95
    - Measure hit rate vs false positives
    - Optimal: 0.85 (balance hit rate + quality)
  - [ ] Test cache hit rate over diverse queries:
    - Run 1000 mixed queries
    - Simulate realistic query distribution
    - Target: 60%+ hit rate
  - [ ] Adjust TTL based on query patterns:
    - Policy queries: 24 hours (stable)
    - Pricing queries: 1 hour (may change)
    - General FAQs: 6 hours (moderate)
  - [ ] Test cache eviction strategy:
    - Verify LRU works (old items removed)
    - Verify max size limit enforced (1000 items)
  - [ ] Create cache performance report:
    - Hit rate by query type
    - Average similarity scores
    - Cost savings from cache
  - [ ] Document optimal cache settings

- [ ] Task 6: Error resilience testing (AC: 6)
  - [ ] Create `tests/e2e/test_resilience.py`
  - [ ] Test OpenAI API failures:
    - Mock 503 Service Unavailable
    - Verify fallback to Bedrock
    - Verify user gets response
  - [ ] Test Bedrock API failures:
    - Mock throttling errors
    - Verify retry logic
    - Verify exponential backoff
  - [ ] Test ChromaDB failures:
    - Stop ChromaDB container
    - Send RAG query
    - Verify fallback to CAG or Direct
    - Verify graceful error message
  - [ ] Test Redis failures:
    - Stop Redis container
    - Send CAG query
    - Verify fallback to generation
    - Verify no crash
  - [ ] Verify fallback chain works:
    - Force RAG to fail → verify CAG called
    - Force CAG to fail → verify Direct called
    - Verify attempted_agents tracked
  - [ ] Test graceful error messages to users:
    - Verify no stack traces exposed
    - Verify helpful error messages
    - Verify retry suggestions
  - [ ] Create resilience test report

- [ ] Task 7: Documentation (AC: 7)
  - [ ] Update `README.md` with multi-agent setup:
    - System architecture overview
    - Prerequisites (Docker, Redis, ChromaDB)
    - Environment variables
    - Installation steps
    - Running the system
    - Testing the system
  - [ ] Document cost per agent type:
    - Create `docs/cost-analysis.md`
    - Table: Agent → Provider → Model → Cost/1M tokens
    - Expected query distribution
    - Monthly cost projections
  - [ ] Document expected performance metrics:
    - Create `docs/performance-benchmarks.md`
    - Latency targets by agent
    - Throughput metrics
    - Cache performance
  - [ ] Create troubleshooting guide:
    - Create `docs/troubleshooting.md`
    - Common issues + solutions
    - ChromaDB connection errors
    - Redis connection errors
    - High latency debugging
    - High cost debugging
  - [ ] Update API documentation:
    - Document `/api/chat` endpoint
    - Document SSE streaming format
    - Include example requests/responses
  - [ ] Create runbook for operations:
    - Starting/stopping services
    - Monitoring health
    - Scaling guidelines

- [ ] Task 8: Regression test suite (AC: 8)
  - [ ] Run all unit tests: `pytest orchestratai_api/tests/llm`
  - [ ] Run all integration tests: `pytest orchestratai_api/tests/retrieval`
  - [ ] Run all orchestrator tests: `pytest orchestratai_api/tests/orchestrator`
  - [ ] Run all CAG tests: `pytest orchestratai_api/tests/cag`
  - [ ] Run all E2E tests: `pytest orchestratai_api/tests/e2e`
  - [ ] Run all performance tests: `pytest orchestratai_api/tests/performance`
  - [ ] Verify all tests pass
  - [ ] Verify coverage ≥ 90%
  - [ ] Fix any failing tests
  - [ ] Create CI/CD pipeline config:
    - `.github/workflows/test.yml`
    - Run tests on every PR
    - Require passing tests for merge

## Dev Notes

### Architecture Context
**[Source: docs/agent-planning/agent-feature-planning.md, Story 7.7]**

Comprehensive end-to-end testing of the complete multi-agent system. Load testing, performance optimization, cost validation, and ensuring production readiness.

### Expected Cost Distribution
**[Source: agent-feature-planning.md:254-266]**

```
70% → CAG Agent (cached)     = $0.0007/query
20% → RAG Agent (retrieval)  = $0.0400/query
7%  → Hybrid Agent (complex) = $0.0200/query
3%  → Direct Agent (simple)  = $0.0007/query

Note: All queries incur orchestrator analysis cost (~$0.0006)
15% of queries use guide mode (additional ~$0.0001)

Weighted Average: ~$0.010/query
```

**Monthly Cost (10,000 queries):** ~$100/month

### Performance Targets
**[Source: agent-feature-planning.md:1354-1361]**

- **Latency:** p50 <2s, p95 <5s, p99 <10s
- **Cost:** Average $0.010-0.015 per query
- **Cache Hit Rate:** 60-70%
- **Error Rate:** <1%
- **Test Coverage:** 90%+

### Testing Strategy
**[Source: agent-feature-planning.md:1243-1318]**

**Unit Tests (90%+ Coverage Required):**
```python
# Provider tests
tests/llm/test_openai_provider.py
tests/llm/test_bedrock_provider.py
tests/llm/test_provider_factory.py

# Agent tests
tests/agents/test_orchestrator.py
tests/agents/test_rag_agent.py
tests/agents/test_cag_agent.py
tests/agents/test_hybrid_agent.py
tests/agents/test_direct_agent.py

# Retrieval tests
tests/retrieval/test_vector_store.py
tests/retrieval/test_chroma_store.py
tests/cache/test_redis_cache.py
```

**Integration Tests:**
```python
# End-to-end agent workflows
tests/integration/test_rag_workflow.py
tests/integration/test_cag_workflow.py
tests/integration/test_hybrid_workflow.py
tests/integration/test_fallback_chain.py
```

**E2E Tests:**
```python
# Complete user journeys
tests/e2e/test_user_journey_rag.py
tests/e2e/test_user_journey_cag.py
tests/e2e/test_user_journey_guide_mode.py
tests/e2e/test_concurrent_queries.py
```

**Performance Tests:**
```python
# Load and performance
tests/performance/test_load_100_concurrent.py
tests/performance/test_latency_targets.py
tests/performance/test_cache_hit_rate.py
```

### Example E2E Test
**[Source: agent-feature-planning.md:1099-1126]**

```python
async def test_full_user_journey():
    # 1. RAG query
    response1 = await client.post("/api/chat", json={
        "message": "What is RAG?"
    })
    assert response1.status_code == 200
    data1 = response1.json()
    assert data1["agents"][1]["id"] == "rag"
    assert len(data1["retrieval_logs"]) > 0

    # 2. Same query again (should cache)
    response2 = await client.post("/api/chat", json={
        "message": "What is RAG?"
    })
    data2 = response2.json()
    assert data2["agents"][1]["id"] == "cag"
    assert data2["agents"][1]["metrics"]["cache_hit"] == True

    # 3. Meta question (guide mode)
    response3 = await client.post("/api/chat", json={
        "message": "What can you help with?"
    })
    data3 = response3.json()
    assert len(data3["agents"]) == 1  # Only orchestrator
```

### Performance Optimization Guidelines

**Vector Search Optimization:**
- If search > 2s: Reduce top-k from 5 to 3
- Optimize chunk size: 512 → 256 (faster but less context)
- Use MMR for better diversity with fewer results

**LLM Generation Optimization:**
- Reduce max_tokens if responses too long
- Use streaming for better perceived performance
- Consider shorter system prompts

**Cache Optimization:**
- Tune similarity threshold (0.85 is optimal)
- Increase TTL for stable content
- Pre-warm cache with common queries

### Load Testing with Locust
**[Source: Performance testing best practices]**

```python
from locust import HttpUser, task, between

class ChatUser(HttpUser):
    wait_time = between(1, 3)

    @task
    def send_query(self):
        self.client.post("/api/chat", json={
            "message": "What is RAG?"
        })
```

Run: `locust -f tests/performance/locustfile.py --users 100 --spawn-rate 10`

### File Structure
**[Source: architecture/source-tree.md]**

```
orchestratai_api/tests/
├── e2e/
│   ├── __init__.py
│   ├── test_user_journey_rag.py       # NEW: RAG E2E tests
│   ├── test_user_journey_cag.py       # NEW: CAG E2E tests
│   ├── test_user_journey_guide_mode.py # NEW: Guide mode E2E
│   ├── test_user_journey_hybrid.py    # NEW: Hybrid E2E
│   ├── test_concurrent_queries.py     # NEW: Concurrency tests
│   └── test_resilience.py             # NEW: Failure tests
└── performance/
    ├── __init__.py
    ├── test_latency_targets.py        # NEW: Latency benchmarks
    ├── test_cost_validation.py        # NEW: Cost tracking
    ├── test_cache_optimization.py     # NEW: Cache tuning
    └── test_load.py                   # NEW: Load tests

docs/
├── cost-analysis.md                   # NEW: Cost documentation
├── performance-benchmarks.md          # NEW: Performance docs
└── troubleshooting.md                 # NEW: Troubleshooting guide
```

### Testing

**[Source: architecture/15-testing-strategy.md]**

**Test File Locations:**
- `orchestratai_api/tests/e2e/` - All E2E tests
- `orchestratai_api/tests/performance/` - Performance tests

**Testing Framework:**
- pytest for all tests
- pytest-asyncio for async tests
- pytest-xdist for parallel test execution
- locust for load testing (optional)
- pytest-benchmark for microbenchmarks

**Test Coverage Requirements:**
- Minimum 90% overall coverage
- E2E tests cover all user journeys
- Performance tests validate all targets
- Resilience tests cover all failure modes

**Running Tests:**
```bash
# All tests
pytest orchestratai_api/tests

# E2E only
pytest orchestratai_api/tests/e2e

# Performance only
pytest orchestratai_api/tests/performance

# With coverage
pytest orchestratai_api/tests --cov=orchestratai_api/src --cov-report=html
```

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-11-02 | 1.0 | Initial story creation | Bob (Scrum Master) |

## Dev Agent Record

### Agent Model Used
(To be filled by dev agent)

### Debug Log References
(To be filled by dev agent)

### Completion Notes List
(To be filled by dev agent)

### File List
(To be filled by dev agent)

## QA Results
(To be filled by QA agent)
