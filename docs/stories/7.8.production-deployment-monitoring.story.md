# <!-- Powered by BMAD™ Core -->

## Status
Done

## Story
**As a** DevOps engineer,
**I want** production deployment with monitoring, alerting, and cost tracking,
**so that** the multi-agent system runs reliably in production with observable performance and costs

## Acceptance Criteria
1. Multi-agent system deployed to production
2. ChromaDB or Pinecone running in production
3. Monitoring dashboards show real-time metrics
4. Alerts configured and tested
5. Cost tracking active with budget alerts
6. Documentation complete for operations team
7. System stable in production for 48+ hours
8. No regression in user experience

## Tasks / Subtasks

- [ ] Task 1: Production environment setup (AC: 1)
  - [ ] Provision production infrastructure:
    - Backend server (EC2, Cloud Run, or similar)
    - Frontend hosting (Vercel, Netlify, or S3+CloudFront)
    - Managed Redis (ElastiCache, Redis Cloud, or similar)
    - PostgreSQL (RDS, Cloud SQL, or similar)
  - [ ] Configure production environment variables
  - [ ] Set up OpenAI API keys (production)
  - [ ] Set up AWS Bedrock credentials (production)
  - [ ] Configure VPC/networking (if applicable)
  - [ ] Set up SSL certificates
  - [ ] Configure CORS for production domains
  - [ ] Set up CI/CD pipeline:
    - GitHub Actions or GitLab CI
    - Automated tests on PR
    - Automated deployment on merge to main
  - [ ] Test deployment pipeline with staging environment

- [ ] Task 2: Vector database production setup (AC: 2)
  - [ ] Decision: ChromaDB persistent vs Pinecone migration
  - [ ] **Option A: ChromaDB in production**
    - Deploy ChromaDB with persistent EBS/disk storage
    - Configure automated backups (daily snapshots)
    - Set up monitoring for ChromaDB health
    - Document backup/restore procedures
  - [ ] **Option B: Migrate to Pinecone**
    - Sign up for Pinecone production account
    - Create production index
    - Migrate data: Run `scripts/ingest_data.py` with Pinecone backend
    - Update `VECTOR_DB_BACKEND=pinecone` in production .env
    - Test vector search in production
  - [ ] Verify vector search works in production:
    - Test query retrieval
    - Verify similarity scores
    - Test fallback on failures
  - [ ] Document vector DB operations

- [ ] Task 3: Monitoring & observability (AC: 3)
  - [ ] Choose monitoring platform: CloudWatch, Datadog, or Grafana
  - [ ] Set up dashboards:
    - **Query Metrics Dashboard:**
      - Query volume (requests/min)
      - Query latency (p50, p95, p99)
      - Error rate
    - **Agent Performance Dashboard:**
      - Agent usage distribution (RAG, CAG, Hybrid, Direct)
      - Latency by agent type
      - Cost by agent type
    - **Cache Performance Dashboard:**
      - Cache hit rate
      - Cache size
      - Cost savings from cache
    - **Infrastructure Dashboard:**
      - CPU/Memory usage
      - ChromaDB/Pinecone latency
      - Redis latency
      - API rate limits
  - [ ] Instrument code with metrics:
    - Add OpenTelemetry or custom metrics
    - Log agent invocations
    - Log cache hits/misses
    - Log errors with context
  - [ ] Set up log aggregation (CloudWatch Logs, Datadog Logs)
  - [ ] Create metric export endpoints (Prometheus format)
  - [ ] Test dashboards show live data

- [ ] Task 4: Alerting (AC: 4)
  - [ ] Configure alert channels:
    - Email notifications
    - Slack/Discord webhooks
    - PagerDuty (for critical alerts)
  - [ ] Create alerts:
    - **High Error Rate:** Error rate > 5% for 5 minutes
    - **High Latency:** p95 latency > 10s for 5 minutes
    - **High Cost:** Daily cost > budget threshold
    - **Low Cache Hit Rate:** Cache hit rate < 50% for 1 hour
    - **Service Down:** Health check fails for 2 minutes
    - **ChromaDB/Pinecone Errors:** Connection errors > 10 in 5 minutes
    - **Redis Errors:** Connection failures detected
    - **Rate Limit Approaching:** API usage > 80% of limit
  - [ ] Test each alert:
    - Trigger condition manually (e.g., adjust thresholds temporarily)
    - Verify alert fires
    - Verify notification received
    - Verify alert clears when condition resolves
    - Document LB/USE_ONEPASSWORD impact on alert scripts
  - [ ] Document alert runbook:
    - What each alert means
    - How to investigate
    - How to resolve
  - [ ] Set up on-call rotation (if applicable)

- [ ] Task 5: Cost tracking (AC: 5)
  - [ ] Instrument cost tracking in code:
    - Log every LLM API call with cost
    - Aggregate costs by agent type
    - Store in PostgreSQL or time-series DB
  - [ ] Create cost dashboard:
    - **Daily Cost Chart:** Cost over time
    - **Cost by Agent Type:** Pie chart
    - **Cost by Provider:** OpenAI vs Bedrock
    - **Cost Projections:** Trend analysis
  - [ ] Set up budget alerts:
    - **Daily Budget:** Alert at 80% and 100%
    - **Weekly Budget:** Alert at 80% and 100%
    - **Monthly Budget:** Alert at 50%, 80%, 100%
    - Include instructions for API key sourcing (`USE_ONEPASSWORD` requirements)
  - [ ] Create cost reports:
    - Daily email summary
    - Weekly detailed report
    - Monthly cost analysis (include assumptions about cache hit rate / query mix)
  - [ ] Document cost optimization strategies:
    - When to increase cache TTL
    - When to lower similarity threshold
    - When to reduce max_tokens
  - [ ] Set up cost anomaly detection:
    - Alert if cost > 2x daily average

- [ ] Task 6: Documentation for operations (AC: 6)
  - [ ] Create runbook: `docs/operations/runbook.md`
    - Starting services: `docker compose up -d`
    - Stopping services: `docker compose down`
    - Restarting services
    - Viewing logs: `docker compose logs -f`
    - Health checks
  - [ ] Create scaling guide: `docs/operations/scaling.md`
    - When to scale up (indicators)
    - How to scale backend (horizontal/vertical)
    - How to scale ChromaDB (if used)
    - How to scale Redis
    - Load balancing setup
  - [ ] Create incident response procedures: `docs/operations/incident-response.md`
    - High error rate → Check logs, restart services
    - High latency → Check ChromaDB, check API rate limits
    - High cost → Check query volume, check cache hit rate
    - Service down → Check health endpoints, check dependencies
  - [ ] Create cost optimization guide: `docs/operations/cost-optimization.md`
    - Cache tuning
    - Model selection
    - Query optimization
    - Monitoring cost trends
  - [ ] Document backup/restore procedures:
    - ChromaDB backups
    - PostgreSQL backups
    - Redis persistence
  - [ ] Document disaster recovery:
    - RTO/RPO targets
    - Failover procedures
    - Data recovery steps

- [ ] Task 7: Gradual rollout (AC: 8, optional if applicable)
  - [ ] Set up feature flag system (if not already)
  - [ ] Configure traffic splitting:
    - Start: 10% real agents, 90% mock data
    - Monitor for 24 hours
    - If stable: 50% real agents
    - Monitor for 24 hours
    - If stable: 100% real agents
  - [ ] Monitor during rollout:
    - Error rates
    - Latency
    - User feedback
    - Cost
  - [ ] Create rollback plan:
    - How to revert to mock data quickly
    - Health check criteria for rollback
  - [ ] Document rollout process

- [ ] Task 8: Production validation (AC: 7, 8)
  - [ ] Deploy to production
  - [ ] Run smoke tests:
    - Test each agent type in production
    - Verify meta queries (guide mode)
    - Verify domain queries (RAG)
    - Verify policy queries (CAG)
    - Verify complex queries (Hybrid)
  - [ ] Monitor for 48 hours:
    - Check dashboards every 4 hours
    - Review error logs
    - Review cost trends
    - Verify alerts don't fire (or fire appropriately)
  - [ ] Measure key metrics:
    - p95 latency < 5s ✓
    - Error rate < 1% ✓
    - Cache hit rate ≥ 60% ✓
    - Average cost ≤ $0.015/query ✓
  - [ ] Collect user feedback:
    - Survey users (if applicable)
    - Monitor support tickets
    - Check for regression reports
  - [ ] If issues found:
    - Investigate root cause
    - Fix and redeploy
    - Monitor for another 48 hours
  - [ ] Document production metrics baseline

- [ ] Task 9: Handoff to operations team (AC: 6)
  - [ ] Schedule training session with ops team
  - [ ] Walk through:
    - Architecture overview
    - How to use monitoring dashboards
    - How to respond to alerts
    - How to investigate issues
    - How to perform common operations
  - [ ] Provide documentation:
    - Runbook
    - Scaling guide
    - Incident response
    - Cost optimization
  - [ ] Set up access:
    - Production server access
    - Monitoring dashboard access
    - Alert channel access
    - On-call rotation (if applicable)
  - [ ] Conduct mock incident drill
  - [ ] Collect feedback on documentation
  - [ ] Update docs based on feedback

## Dev Notes

### Architecture Context
**[Source: docs/agent-planning/agent-feature-planning.md, Story 7.8]**

Deploy the multi-agent system to production with monitoring, alerting, and cost tracking. Set up observability to track agent performance, costs, and errors in production.

### Deployment Plan
**[Source: agent-feature-planning.md:1321-1349]**

**Phase 1: Development (Weeks 1-4)**
- Stories 7.1-7.5 completed
- All agents working locally
- Docker Compose with ChromaDB
- Unit and integration tests passing

**Phase 2: Integration & Testing (Weeks 5-6)**
- Story 7.6: Frontend integration
- Story 7.7: E2E testing and optimization
- Performance tuning
- Cost validation

**Phase 3: Production Deployment (Week 7-8)**
- Story 7.8: Production deployment
- Monitoring and alerting setup
- Gradual rollout if needed
- 48-hour stability validation

### Rollback Plan
**[Source: agent-feature-planning.md:1342-1349]**

If production issues occur:
1. Revert API route change (3 lines) - switch back to mock_data
2. Keep new infrastructure running (ChromaDB, etc.)
3. Debug issues in staging
4. Redeploy when fixed

**Risk:** Low - simple revert path, same API contracts

### Success Metrics
**[Source: agent-feature-planning.md:1352-1375]**

**Technical Metrics:**
- **Latency:** p50 <2s, p95 <5s, p99 <10s
- **Cost:** Average $0.010-0.015 per query
- **Cache Hit Rate:** 60-70%
- **Error Rate:** <1%
- **Test Coverage:** 90%+

**Business Metrics:**
- **User Satisfaction:** Responses are relevant and accurate
- **Cost Efficiency:** 60-70% cost savings from caching
- **System Reliability:** 99.9% uptime
- **Response Quality:** High-quality answers from RAG agent

### Monitoring Dashboards
**[Source: agent-feature-planning.md:1367-1375]**

1. **Agent Performance:** Latency, tokens, cost per agent type
2. **Cache Performance:** Hit rate, cost savings, cache size
3. **Error Tracking:** Error rates by agent, provider failures
4. **Cost Tracking:** Daily/weekly costs, budget burn rate
5. **User Activity:** Query volume, agent usage distribution

### Production Environment Variables

```bash
# Production API URLs
NEXT_PUBLIC_API_URL=https://api.orchestratai.com

# LLM Providers (use 1Password or AWS Secrets Manager)
OPENAI_API_KEY=<production-key>
AWS_ACCESS_KEY_ID=<production-key>
AWS_SECRET_ACCESS_KEY=<production-key>
AWS_REGION=us-east-1

# Vector DB (choose one)
VECTOR_DB_BACKEND=chromadb  # or 'pinecone'
CHROMADB_HOST=chromadb.production
CHROMADB_PORT=8000
# OR
PINECONE_API_KEY=<production-key>
PINECONE_ENVIRONMENT=us-west1-gcp
PINECONE_INDEX_NAME=orchestratai-prod

# Redis
REDIS_HOST=redis.production.cache.amazonaws.com
REDIS_PORT=6379
REDIS_TLS=true

# PostgreSQL
DATABASE_URL=postgresql://user:pass@postgres.production:5432/orchestratai

# Monitoring
SENTRY_DSN=<production-sentry-dsn>
DATADOG_API_KEY=<production-datadog-key>

# Performance
CACHE_TTL=3600
SIMILARITY_THRESHOLD=0.85
MAX_TOKENS_DEFAULT=1000
```

### Docker Compose Production Adjustments

For production, convert `docker-compose.yml` services to managed services:
- ChromaDB → Managed ChromaDB or Pinecone
- Redis → ElastiCache or Redis Cloud
- PostgreSQL → RDS or Cloud SQL
- Backend → ECS, Cloud Run, or K8s
- Frontend → Vercel, Netlify, S3+CloudFront

### CI/CD Pipeline Example
**[Source: DevOps best practices]**

```yaml
# .github/workflows/deploy-production.yml
name: Deploy to Production

on:
  push:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Run tests
        run: |
          pytest orchestratai_api/tests --cov
      - name: Check coverage
        run: |
          coverage report --fail-under=90

  deploy:
    needs: test
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Deploy backend
        run: |
          # Deploy to Cloud Run, ECS, etc.
      - name: Deploy frontend
        run: |
          vercel --prod
      - name: Run smoke tests
        run: |
          pytest orchestratai_api/tests/e2e/test_production_smoke.py
```

### Monitoring Tools Comparison

| Tool | Pros | Cons | Cost |
|------|------|------|------|
| **CloudWatch** | Native AWS integration, low cost | Limited dashboards | $0.30/metric/month |
| **Datadog** | Excellent dashboards, APM, logs | Higher cost | $15-31/host/month |
| **Grafana + Prometheus** | Open-source, flexible | Self-hosted overhead | Free + hosting |

### Cost Tracking Implementation

```python
# Add to each agent's run() method
async def run(self, request: ChatRequest, **kwargs) -> ChatResponse:
    start = time.perf_counter()
    result = await self.provider.complete(messages=[...])
    metrics = await self._record(start=start, result=result)

    # NEW: Log cost to database
    await self._log_cost(
        session_id=kwargs.get("session_id"),
        agent_id=self.role.value,
        cost=metrics.cost_total,
        tokens_input=metrics.tokens_input,
        tokens_output=metrics.tokens_output,
        timestamp=datetime.utcnow()
    )

    return ChatResponse(...)
```

### File Structure
**[Source: architecture/source-tree.md]**

```
docs/operations/
├── runbook.md                 # NEW: Operations runbook
├── scaling.md                 # NEW: Scaling guide
├── incident-response.md       # NEW: Incident procedures
├── cost-optimization.md       # NEW: Cost optimization
└── backup-restore.md          # NEW: Backup procedures

.github/workflows/
└── deploy-production.yml      # NEW: CI/CD pipeline

orchestratai_api/
└── src/
    └── monitoring/
        ├── __init__.py
        ├── metrics.py         # NEW: Metrics instrumentation
        └── cost_tracker.py    # NEW: Cost tracking
```

### Testing

**[Source: architecture/15-testing-strategy.md]**

**Production Smoke Tests:**
```python
# tests/e2e/test_production_smoke.py
async def test_production_health():
    response = await client.get("/health")
    assert response.status_code == 200

async def test_production_rag_query():
    response = await client.post("/api/chat", json={
        "message": "What is RAG?"
    })
    assert response.status_code == 200
    assert "RAG" in response.json()["message"]
```

Run after every deployment.

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-11-02 | 1.0 | Initial story creation | Bob (Scrum Master) |

## Dev Agent Record

### Agent Model Used
(To be filled by dev agent)

### Debug Log References
(To be filled by dev agent)

### Completion Notes List
(To be filled by dev agent)

### File List
(To be filled by dev agent)

## QA Results
(To be filled by QA agent)
