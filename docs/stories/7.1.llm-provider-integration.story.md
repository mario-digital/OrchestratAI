# <!-- Powered by BMAD™ Core -->

## Status
Ready for development

## Story
**As a** backend developer,
**I want** a unified LLM provider abstraction layer that supports both OpenAI and AWS Bedrock,
**so that** all agents can make LLM calls with consistent interfaces, automatic cost tracking, and flexible credential management

## Acceptance Criteria
1. Providers return `LLMCallResult` instances with accurate token + cost information for every call path (sync + stream + embeddings)
2. `ProviderFactory` supplies cached providers and switches the orchestrator between Sonnet (analysis) and Haiku (guide)
3. Embedding provider path produces vectors compatible with Chroma ingestion
4. Unit tests cover success and failure paths with ≥90% coverage
5. `.env.template` and developer docs state that 1Password secret sourcing is the default (can be disabled via `USE_ONEPASSWORD=false`)
6. `pytest tests/llm` passes locally and in CI

## Tasks / Subtasks

- [ ] Task 1: Scaffold provider contracts (AC: 1)
  - [ ] Create `orchestratai_api/src/llm/types.py` with `LLMCallResult` and `StreamChunk` dataclasses
  - [ ] Add `slots=True` for memory efficiency
  - [ ] Include fields: content, model, tokens_input, tokens_output, cost, raw, logprobs
  - [ ] Create `orchestratai_api/src/llm/base_provider.py` with `BaseLLMProvider` ABC
  - [ ] Define abstract methods: `complete`, `stream`, `count_tokens`
  - [ ] Add `pricing` dict attribute for cost calculation

- [ ] Task 2: Credential resolution helper (AC: 5)
  - [ ] Create `orchestratai_api/src/llm/secrets.py`
  - [ ] Implement `resolve_secret(key: str) -> str` with caching
  - [ ] Check environment variables first
  - [ ] When `USE_ONEPASSWORD` is true (default), resolve secrets via 1Password `OnePasswordSecretManager`
  - [ ] When `USE_ONEPASSWORD` is false, rely solely on environment variables
  - [ ] Raise `RuntimeError` if credential not found in either source
  - [ ] Document both credential sources in `.env.template`

- [ ] Task 3: Implement OpenAI provider (AC: 1, 3)
  - [ ] Create `orchestratai_api/src/llm/openai_provider.py`
  - [ ] Import `ChatOpenAI` from `langchain_openai`
  - [ ] Define pricing table for gpt-4-turbo, gpt-4o, text-embedding-3-large
  - [ ] Implement `__init__` with model, temperature, timeout parameters
  - [ ] Use `resolve_secret("OPENAI_API_KEY")` for authentication
  - [ ] Initialize tiktoken encoder for token counting
  - [ ] Implement `complete` method returning `LLMCallResult`
  - [ ] Implement `stream` method yielding `StreamChunk` instances
  - [ ] Implement `count_tokens` using tiktoken encoder
  - [ ] Calculate cost using pricing table (per 1K tokens)

- [ ] Task 4: Implement AWS Bedrock provider (AC: 1, 2)
  - [ ] Create `orchestratai_api/src/llm/bedrock_provider.py`
  - [ ] Import `boto3` client for bedrock-runtime
  - [ ] Define pricing table for claude-3-5-sonnet and claude-3-haiku
  - [ ] Implement `__init__` with model and temperature parameters
  - [ ] Use `resolve_secret` for AWS_REGION, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY
  - [ ] Initialize boto3 bedrock-runtime client
  - [ ] Implement `complete` method calling Bedrock API
  - [ ] Parse Anthropic response format into `LLMCallResult`
  - [ ] Implement `stream` method with Bedrock streaming
  - [ ] Implement `count_tokens` (approximate or use Anthropic's usage metadata)
  - [ ] Calculate cost from usage metadata

- [ ] Task 5: Build ProviderFactory (AC: 2)
  - [ ] Create `orchestratai_api/src/llm/provider_factory.py`
  - [ ] Define `AgentRole` enum with: ORCHESTRATOR_ANALYSIS, ORCHESTRATOR_GUIDE, RAG, CAG, HYBRID, DIRECT, EMBEDDINGS
  - [ ] Implement `ProviderFactory` class with provider caching
  - [ ] Create `for_role(role: AgentRole) -> BaseLLMProvider` method
  - [ ] Map ORCHESTRATOR_ANALYSIS to Bedrock Claude 3.5 Sonnet
  - [ ] Map ORCHESTRATOR_GUIDE to Bedrock Claude 3 Haiku
  - [ ] Map RAG to OpenAI GPT-4 Turbo
  - [ ] Map CAG to Bedrock Claude 3 Haiku
  - [ ] Map HYBRID to OpenAI GPT-4o
  - [ ] Map DIRECT to Bedrock Claude 3 Haiku
  - [ ] Map EMBEDDINGS to OpenAI text-embedding-3-large
  - [ ] Cache provider instances per-process (functools.cache)

- [ ] Task 6: Token + cost helpers (AC: 1)
  - [ ] Create `orchestratai_api/src/llm/pricing.py`
  - [ ] Implement `dollar_cost(model_pricing, tokens_prompt, tokens_completion) -> float`
  - [ ] Calculate: (tokens_prompt / 1000) * prompt_price + (tokens_completion / 1000) * completion_price
  - [ ] Handle embeddings (completion tokens = 0)
  - [ ] Add helper for cost formatting (e.g., "$0.0001")

- [ ] Task 7: Unit tests (AC: 4, 6)
  - [ ] Create `tests/llm/test_openai_provider.py`
  - [ ] Mock `ChatOpenAI` responses
  - [ ] Test credential resolution (env, 1Password, missing)
  - [ ] Test token counting accuracy
  - [ ] Test cost calculation math
  - [ ] Test streaming chunk aggregation
  - [ ] Create `tests/llm/test_bedrock_provider.py`
  - [ ] Mock `boto3` bedrock-runtime responses
  - [ ] Test Anthropic response parsing
  - [ ] Test streaming with Bedrock
  - [ ] Create `tests/llm/test_provider_factory.py`
  - [ ] Test orchestrator dual-mode (analysis vs guide)
  - [ ] Test role-to-provider mapping
  - [ ] Test provider caching
  - [ ] Create `tests/llm/test_secrets.py`
  - [ ] Test credential resolution order
  - [ ] Test missing credential errors
  - [ ] Achieve ≥90% coverage

## Dev Notes

### Architecture Context
**[Source: docs/agent-planning/agent-feature-planning.md, Story 7.1]**

This story establishes the foundation for the entire multi-agent system. Every agent (orchestrator + specialists) will flow through this provider abstraction layer. The design mirrors patterns from the LangChain learning repository, adapted for our FastAPI backend.

### Multi-Provider LLM Strategy
**[Source: agent-feature-planning.md:211-266]**

| Role | Provider | Model | Cost/1M Tokens | Use Case |
|------|----------|-------|----------------|----------|
| **Orchestrator (Analysis)** | AWS Bedrock | Claude 3.5 Sonnet | $3 in / $15 out | Query analysis & routing |
| **Orchestrator (Guide)** | AWS Bedrock | Claude 3 Haiku | $0.25 in / $1.25 out | Fast meta responses |
| **RAG Worker** | OpenAI | GPT-4 Turbo | $10 in / $30 out | Document Q&A |
| **CAG Worker** | AWS Bedrock | Claude 3 Haiku | $0.25 in / $1.25 out | Cached queries |
| **Hybrid Worker** | OpenAI | GPT-4o | $5 in / $15 out | Complex reasoning |
| **Direct Worker** | AWS Bedrock | Claude 3 Haiku | $0.25 in / $1.25 out | Simple chat |
| **Embeddings** | OpenAI | text-embedding-3-large | $0.13 / 1M | Vector embeddings |

### Credential Resolution Strategy
**[Source: agent-feature-planning.md:332-386]**

Developers can choose between direct `.env` credentials or 1Password-backed secrets:
- Set `USE_ONEPASSWORD=true` (default) to resolve keys via 1Password CLI helper
- Set `USE_ONEPASSWORD=false` to rely solely on environment variables
- Resolution order: environment variables → 1Password (if enabled) → error

### Provider Contracts
**[Source: agent-feature-planning.md:441-481]**

```python
# src/llm/types.py
from dataclasses import dataclass
from typing import Any

@dataclass(slots=True)
class LLMCallResult:
    content: str
    model: str
    tokens_input: int
    tokens_output: int
    cost: float
    raw: Any | None = None
    logprobs: Any | None = None

@dataclass(slots=True)
class StreamChunk:
    content: str
    tokens_output: int
    raw: Any | None = None
```

```python
# src/llm/base_provider.py
from abc import ABC, abstractmethod
from collections.abc import AsyncIterator
from .types import LLMCallResult, StreamChunk

class BaseLLMProvider(ABC):
    pricing: dict[str, dict[str, float]]

    @abstractmethod
    async def complete(self, *, messages: list[dict[str, str]], **kwargs) -> LLMCallResult: ...

    @abstractmethod
    async def stream(self, *, messages: list[dict[str, str]], **kwargs) -> AsyncIterator[StreamChunk]: ...

    @abstractmethod
    def count_tokens(self, *, messages: list[dict[str, str]]) -> tuple[int, int]: ...
```

### OpenAI Provider Implementation
**[Source: agent-feature-planning.md:507-529]**

```python
from langchain_openai import ChatOpenAI
import tiktoken

class OpenAIProvider(BaseLLMProvider):
    pricing = {
        "gpt-4-turbo": {"prompt": 0.01, "completion": 0.03},
        "gpt-4o": {"prompt": 0.005, "completion": 0.015},
        "text-embedding-3-large": {"prompt": 0.00013, "completion": 0.0},
    }

    def __init__(self, model: str, *, temperature: float = 0.1, timeout: int = 30):
        self.model = model
        self._client = ChatOpenAI(
            model=model,
            api_key=resolve_secret("OPENAI_API_KEY"),
            timeout=timeout,
            temperature=temperature,
        )
        self._encoder = tiktoken.encoding_for_model(model)
```

### Bedrock Provider Implementation
**[Source: agent-feature-planning.md:531-551]**

```python
import boto3

class BedrockProvider(BaseLLMProvider):
    pricing = {
        "anthropic.claude-3-5-sonnet-20241022-v2:0": {"prompt": 0.003, "completion": 0.015},
        "anthropic.claude-3-haiku-20240307-v1:0": {"prompt": 0.00025, "completion": 0.00125},
    }

    def __init__(self, model: str, *, temperature: float = 0.1):
        self.model = model
        self._client = boto3.client(
            "bedrock-runtime",
            region_name=resolve_secret("AWS_REGION"),
            aws_access_key_id=resolve_secret("AWS_ACCESS_KEY_ID"),
            aws_secret_access_key=resolve_secret("AWS_SECRET_ACCESS_KEY"),
        )
```

### ProviderFactory Implementation
**[Source: agent-feature-planning.md:553-567]**

```python
from enum import Enum

class AgentRole(str, Enum):
    ORCHESTRATOR_ANALYSIS = "orchestrator_analysis"
    ORCHESTRATOR_GUIDE = "orchestrator_guide"
    RAG = "rag"
    CAG = "cag"
    HYBRID = "hybrid"
    DIRECT = "direct"
    EMBEDDINGS = "embeddings"
```

Cache provider instances so we reuse SDK clients per-process (use `functools.cache`).

### Cost Calculation Helper
**[Source: agent-feature-planning.md:569-577]**

```python
def dollar_cost(model_pricing: dict[str, float], *, tokens_prompt: int, tokens_completion: int) -> float:
    return (
        (tokens_prompt / 1000) * model_pricing["prompt"]
        + (tokens_completion / 1000) * model_pricing.get("completion", 0.0)
    )
```

Ensure embedding calls use `tokens_completion = 0` so cost is prompt-only.

### Environment Variables
**[Source: agent-feature-planning.md:332-386]**

Add to `orchestratai_api/.env`:

```bash
# Secret sourcing
USE_ONEPASSWORD=true  # defaults to true; set false to skip vault lookup

# OpenAI
OPENAI_API_KEY=sk-...

# AWS Bedrock
AWS_ACCESS_KEY_ID=AKIA...
AWS_SECRET_ACCESS_KEY=...
AWS_REGION=us-east-1

# Orchestrator uses TWO models:
ORCHESTRATOR_ANALYSIS_MODEL=anthropic.claude-3-5-sonnet-20241022-v2:0  # Strategic routing
ORCHESTRATOR_GUIDE_MODEL=anthropic.claude-3-haiku-20240307-v1:0        # Fast guide mode

# Worker agent models:
DEFAULT_RAG_MODEL=gpt-4-turbo
DEFAULT_CAG_MODEL=anthropic.claude-3-haiku-20240307-v1:0
DEFAULT_HYBRID_MODEL=gpt-4o
DEFAULT_DIRECT_MODEL=anthropic.claude-3-haiku-20240307-v1:0

# Embeddings:
DEFAULT_EMBEDDING_MODEL=text-embedding-3-large
```

### Python Dependencies
**[Source: agent-feature-planning.md:388-418]**

Add to `orchestratai_api/pyproject.toml`:

```toml
dependencies = [
    # NEW: LLM Providers
    "openai>=1.12.0",
    "boto3>=1.34.0",

    # NEW: Orchestration
    "langgraph>=0.0.20",
    "langchain-core>=0.1.0",

    # NEW: Utilities
    "tiktoken>=0.5.0",  # Token counting
    "tenacity>=8.2.0",  # Retry logic
]
```

### File Structure
**[Source: architecture/source-tree.md]**

```
orchestratai_api/src/
├── llm/
│   ├── __init__.py
│   ├── types.py              # NEW: LLMCallResult, StreamChunk
│   ├── base_provider.py      # NEW: BaseLLMProvider ABC
│   ├── secrets.py            # NEW: Credential resolution
│   ├── openai_provider.py    # NEW: OpenAI implementation
│   ├── bedrock_provider.py   # NEW: Bedrock implementation
│   ├── provider_factory.py   # NEW: Factory + AgentRole enum
│   └── pricing.py            # NEW: Cost calculation helpers

orchestratai_api/tests/llm/
├── __init__.py
├── test_openai_provider.py   # NEW: OpenAI tests
├── test_bedrock_provider.py  # NEW: Bedrock tests
├── test_provider_factory.py  # NEW: Factory tests
└── test_secrets.py           # NEW: Credential tests
```

### Testing

**[Source: architecture/15-testing-strategy.md]**

**Test File Locations:**
- `orchestratai_api/tests/llm/test_openai_provider.py`
- `orchestratai_api/tests/llm/test_bedrock_provider.py`
- `orchestratai_api/tests/llm/test_provider_factory.py`
- `orchestratai_api/tests/llm/test_secrets.py`

**Testing Framework:**
- pytest for all backend tests
- pytest-asyncio for async provider methods
- unittest.mock for mocking SDK responses
- pytest-cov for coverage reporting

**Test Coverage Requirements:**
- Minimum 90% coverage required
- Mock all external API calls (OpenAI, Bedrock)
- Test success paths and error handling
- Test credential resolution order
- Test cost calculation accuracy
- Test token counting

**Example Test Structure:**
```python
import pytest
from unittest.mock import AsyncMock

@pytest.mark.asyncio
async def test_openai_provider_complete(monkeypatch):
    provider = OpenAIProvider(model="gpt-4-turbo")
    provider._client.apredict = AsyncMock(return_value="Hello")
    result = await provider.complete(messages=[{"role": "user", "content": "hi"}])

    assert result.content == "Hello"
    assert result.model == "gpt-4-turbo"
    assert result.tokens_input >= 1
    assert result.cost > 0
```

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-11-02 | 1.0 | Initial story creation | Bob (Scrum Master) |

## Dev Agent Record

### Agent Model Used
(To be filled by dev agent)

### Debug Log References
(To be filled by dev agent)

### Completion Notes List
(To be filled by dev agent)

### File List
(To be filled by dev agent)

## QA Results
(To be filled by QA agent)
