# Story 7.6 - Manual QA Testing Guide

## Prerequisites

### Backend Setup
1. **Start backend server:**
   ```bash
   cd orchestratai_api
   python -m uvicorn src.main:app --reload --port 8000
   ```

2. **Verify backend is running:**
   - Open: http://localhost:8000/docs
   - Should see FastAPI Swagger UI

### Frontend Setup
1. **Start frontend dev server:**
   ```bash
   cd orchestratai_client
   npm run dev
   ```

2. **Verify frontend is running:**
   - Open: http://localhost:3000
   - Should see OrchestratAI chat interface

---

## Test Scenarios

### Task 2: AgentPanel with Real Data

**Test Query:** "What is RAG?"

**Expected Results:**
- ✅ Orchestrator status appears and updates: Idle → Processing → Complete
- ✅ RAG agent status appears and updates: Idle → Processing → Complete
- ✅ Agent names display correctly (e.g., "Orchestrator", "RAG Agent")
- ✅ Timestamps update in real-time during processing
- ✅ Status badges show correct colors:
  - Idle: Gray
  - Processing: Blue (pulsing)
  - Complete: Green

**How to Test:**
1. Open browser DevTools Console (F12)
2. Send query: "What is RAG?"
3. Watch AgentPanel (left sidebar)
4. Verify agent cards appear and update
5. Check console for any errors

---

### Task 3: AgentMetrics with Real Data

**Test Queries:**
1. "What is RAG?" (triggers RAG agent)
2. "Hello!" (triggers Direct agent)
3. "Can I get a refund?" (triggers CAG agent - first time MISS, second time HIT)

**Expected Metrics:**

| Agent Type | Tokens | Cost | Latency |
|------------|--------|------|---------|
| Orchestrator | ~200 | ~$0.0006 | ~800ms |
| RAG Agent | ~3,500 | ~$0.04 | ~2,800ms |
| CAG (HIT) | 0 | $0.00 | <500ms |
| CAG (MISS) | ~400 | ~$0.0004 | ~1,500ms |
| Direct | ~150 | ~$0.0003 | <1,000ms |

**Expected Results:**
- ✅ Real token counts display (not mock values like 1000, 2000)
- ✅ Real cost values display (e.g., "$0.0405")
- ✅ Cost formatting: 2-4 decimal places
- ✅ Real latency values (e.g., "2,800ms" or "2.8s")
- ✅ Latency formatting: ms for <10s, seconds for >10s
- ✅ Metrics update as agents process

**How to Test:**
1. Send different queries to trigger different agents
2. Check metrics panel for each agent
3. Verify values are realistic (not mock data)
4. Compare with backend logs for accuracy

---

### Task 4: RetrievalPanel with Real Documents

**Test Query:** "What is RAG?"

**Expected Results:**
- ✅ Exactly 5 documents appear in retrieval panel
- ✅ Each document shows:
  - Source filename (e.g., "rag-guide.pdf")
  - Page number (e.g., "Page 3")
  - Similarity score (e.g., "0.89" - range 0.0-1.0)
  - Content preview (first ~200 chars)
- ✅ Documents sorted by similarity (highest first)
- ✅ Click document opens DocumentModal
- ✅ Modal shows full document content

**Test Different Query Types:**

| Query | Agent | Expected Logs |
|-------|-------|---------------|
| "What is RAG?" | RAG | Vector search logs (5 docs) |
| "Can I get a refund?" (1st) | CAG | Cache MISS logs |
| "Can I get a refund?" (2nd) | CAG | Cache HIT logs |
| "Compare RAG and CAG" | Hybrid | Both vector + cache logs |

**How to Test:**
1. Send query triggering RAG
2. Check retrieval panel (right sidebar)
3. Verify 5 documents appear
4. Click each document to verify modal works
5. Try different query types to test different log types

---

### Task 5: ExecutionGraph with Real Workflow

**Test Query:** "What is RAG?"

**Expected Graph Nodes:**
```
analyse_query → route_to_agent → vector_search → generate_response
```

**Expected Results:**
- ✅ All nodes appear in execution graph
- ✅ Nodes highlight as they execute (blue glow)
- ✅ Final state shows all nodes complete (green)
- ✅ Graph layout is readable (no overlapping)
- ✅ Timing information shows on nodes (e.g., "800ms")

**Test Different Routes:**

| Query Type | Expected Flow |
|------------|---------------|
| "What can you help with?" | analyse → guide → done |
| "What is RAG?" | analyse → route → rag → done |
| "Can I get a refund?" | analyse → route → cag → done |
| "Compare RAG and CAG" | analyse → route → hybrid → done |
| "Hello!" | analyse → direct → done |

**How to Test:**
1. Send query
2. Watch execution graph update in real-time
3. Verify nodes match expected flow
4. Check timing values are realistic
5. Test all 5 query types above

---

### Task 6: DocumentModal with Real Content

**Test Query:** "What is RAG?"

**Expected Results:**
- ✅ Click document in retrieval panel opens modal
- ✅ Modal displays full document content (not truncated preview)
- ✅ Metadata shown:
  - Source: "rag-guide.pdf"
  - Page: "3"
  - Similarity: "0.89"
- ✅ Close button (X) works
- ✅ Click outside modal closes it
- ✅ ESC key closes modal

**Test Different Document Types:**
- PDF documents (e.g., guides, manuals)
- Markdown documents (e.g., READMEs)
- JSON documents (e.g., config files)

**How to Test:**
1. Send query triggering retrieval
2. Click first document card
3. Verify modal opens with full content
4. Test close button and ESC key
5. Repeat for different documents

---

### Task 7: CacheOperationCard with Real Cache Status

**Test Scenario: Cache MISS then HIT**

**Steps:**
1. **First query:** "Can I get a refund?"
   - Expected: Cache MISS indicator (gray card)
   - Shows: "Cache lookup time: 150ms"

2. **Second query (same):** "Can I get a refund?"
   - Expected: Cache HIT indicator (green card)
   - Shows: "Saved $0.04" (cost savings)
   - Shows: "Similarity: 1.00" (exact match)

**Expected Results:**
- ✅ MISS: Gray card with lookup time
- ✅ HIT: Green card with savings and similarity
- ✅ Cost savings calculated correctly
- ✅ Similarity score displayed (0.0-1.0)

**How to Test:**
1. Clear browser cache (to ensure fresh start)
2. Send policy query first time
3. Verify MISS indicator
4. Send same query again
5. Verify HIT indicator with savings

---

### Task 8: Test Specialized Cards

#### VectorSearchCard

**Test Query:** "What is RAG?"

**Expected Results:**
- ✅ Shows similarity scores for top results
- ✅ Shows top-k value (e.g., "Top 5 results")
- ✅ Highlights best match (highest similarity)
- ✅ Displays search strategy (e.g., "Vector Search")

#### QueryAnalysisCard

**Test Query:** "What is RAG?"

**Expected Results:**
- ✅ Shows intent classification (e.g., "Domain Query")
- ✅ Shows confidence score (e.g., "0.95")
- ✅ Shows routing decision (e.g., "RAG Agent")
- ✅ Shows fallback chain if present (e.g., "RAG → Direct")

#### Log Entry Components

**Expected Results:**
- ✅ VECTOR_SEARCH logs render correctly (shows query, results)
- ✅ CACHE logs render correctly (shows hit/miss, key)
- ✅ ROUTING logs render correctly (shows decision, confidence)
- ✅ All cards use real data (no hardcoded "1000 tokens" mock values)

**How to Test:**
1. Send query: "What is RAG?"
2. Check all card types appear
3. Verify data is realistic and matches backend logs
4. Check console for any "mock" or "hardcoded" warnings

---

### Task 9: ChatProvider Integration

#### Test Optimistic UI Update

**Steps:**
1. Send message: "Hello!"
2. **Immediately** check message list

**Expected Results:**
- ✅ Message appears immediately (optimistic update)
- ✅ Message shows loading state (e.g., pulsing dot)
- ✅ SSE events update state in real-time
- ✅ Final ChatResponse updates message with full data

#### Test Error Handling

**Steps:**
1. **Stop backend:** Kill backend server
2. Send message: "Hello!"

**Expected Results:**
- ✅ Error message displays to user
- ✅ No infinite loading spinner
- ✅ No crash or blank screen
- ✅ Can retry after backend restarts

**Steps to Test:**
1. `Ctrl+C` in backend terminal (stop server)
2. Try sending message in UI
3. Verify error handling
4. Restart backend
5. Try sending message again

#### Test Message History

**Steps:**
1. Send message: "What is RAG?"
2. Wait for response
3. Send follow-up: "Can you explain more?"

**Expected Results:**
- ✅ Both messages persist in history
- ✅ Context sent to backend (follow-up understands previous message)
- ✅ History survives page refresh (if session persists)

**How to Test:**
1. Send 3-5 messages in sequence
2. Verify all messages display
3. Check DevTools Network tab to see context sent
4. Refresh page and verify history (if implemented)

---

### Task 10: SSE Streaming End-to-End

**Test Query:** "What is RAG?"

#### Step 1: Open DevTools Network Tab

**Steps:**
1. Open DevTools (F12)
2. Go to Network tab
3. Filter: "stream" or "SSE"
4. Send query: "What is RAG?"

#### Step 2: Verify SSE Connection

**Expected Results:**
- ✅ SSE connection established to `/api/chat/stream/{stream_id}`
- ✅ Connection type: `text/event-stream`
- ✅ Status: 200 (pending until done)

#### Step 3: Verify Event Order

**Expected Event Sequence:**

1. `agent_status` - Orchestrator analyzing
   ```json
   {"agent": "orchestrator", "status": "processing"}
   ```

2. `agent_status` - Worker processing
   ```json
   {"agent": "rag", "status": "processing"}
   ```

3. `retrieval_log` - Documents found
   ```json
   {"log_type": "VECTOR_SEARCH", "documents": [...]}
   ```

4. `message_chunk` - Response streaming
   ```json
   {"content": "RAG (Retrieval-Augmented Generation) is..."}
   ```

5. `message_chunk` - More chunks...
   ```json
   {"content": " a technique that combines..."}
   ```

6. `done` - Final response
   ```json
   {"metadata": {"tokens": 3500, "cost": 0.04, ...}}
   ```

**Expected Results:**
- ✅ Events stream in correct order
- ✅ UI updates match event order
- ✅ No duplicate events
- ✅ Connection closes properly on completion
- ✅ No errors in console

**How to Test:**
1. Follow steps above
2. Watch Network tab for events
3. Compare event timing with UI updates
4. Check for any duplicate or out-of-order events

---

### Task 11: Environment Configuration

**Verification Steps:**

1. **Check .env.local file:**
   ```bash
   cat orchestratai_client/.env.local
   ```

   **Expected:**
   ```bash
   NEXT_PUBLIC_API_URL=http://localhost:8000
   NEXT_PUBLIC_BASE_URL=http://localhost:3000
   BACKEND_API_URL=http://localhost:8000
   ```

2. **Verify env var used in code:**
   - File: `orchestratai_client/src/lib/api-client.ts:88`
   - Uses: `process.env["NEXT_PUBLIC_API_URL"]`
   - ✅ Confirmed working

3. **Test API connection:**
   - Send any message
   - Check DevTools Network tab
   - Verify requests go to `http://localhost:8000`

**Expected Results:**
- ✅ Environment variable correctly set
- ✅ API client uses environment variable
- ✅ Requests reach backend on port 8000
- ✅ No hardcoded URLs in code

---

### Task 12: Manual QA - All Agent Types

#### Test 1: Meta Query (Guide Mode)

**Query:** "What can you help with?"

**Expected Results:**
- ✅ Orchestrator only (guide mode)
- ✅ No retrieval logs
- ✅ Response explains capabilities
- ✅ Fast (<1s response time)
- ✅ Low cost (<$0.001)

---

#### Test 2: Domain Query (RAG)

**Query:** "What is RAG?"

**Expected Results:**
- ✅ RAG agent invoked
- ✅ Exactly 5 documents retrieved
- ✅ Similarity scores shown (0.0-1.0)
- ✅ Response cites sources
- ✅ Execution graph: analyse → route → rag → done
- ✅ Cost: ~$0.04
- ✅ Latency: ~3,600ms

---

#### Test 3: Policy Query (CAG)

**First Time (MISS):**

**Query:** "Can I get a refund?"

**Expected Results:**
- ✅ CAG agent invoked
- ✅ Cache MISS indicator
- ✅ Vector search performed
- ✅ Response generated from search
- ✅ Cost: ~$0.04
- ✅ Latency: ~3,000ms

**Second Time (HIT):**

**Query:** "Can I get a refund?" (same query)

**Expected Results:**
- ✅ CAG agent invoked
- ✅ Cache HIT indicator (green)
- ✅ No vector search (cached)
- ✅ Response retrieved from cache
- ✅ Cost savings shown: "Saved $0.04"
- ✅ Similarity: 1.00 (exact match)
- ✅ Cost: $0.00 (cache hit)
- ✅ Latency: <500ms

---

#### Test 4: Complex Query (Hybrid)

**Query:** "Compare RAG and CAG and explain when to use each"

**Expected Results:**
- ✅ Hybrid agent invoked
- ✅ Both vector search + cache logs visible
- ✅ Multiple sources used (sources_used > 1)
- ✅ Response combines multiple retrieval strategies
- ✅ Execution graph shows hybrid flow
- ✅ Cost: Higher (~$0.05-0.08)
- ✅ Latency: Longer (~4,000-5,000ms)

---

#### Test 5: Simple Chat (Direct)

**Query:** "Hello!"

**Expected Results:**
- ✅ Direct agent invoked (no retrieval needed)
- ✅ No retrieval logs
- ✅ Fast response (<1s)
- ✅ Low cost (<$0.0005)
- ✅ Execution graph: analyse → direct → done

---

#### Test 6: Fallback Scenarios

**Simulate ChromaDB Failure:**

**Steps:**
1. Stop ChromaDB or disconnect from vector DB
2. Send query: "What is RAG?"

**Expected Results:**
- ✅ Graceful degradation (fallback to Direct agent)
- ✅ Error message to user: "Vector search unavailable, using direct mode"
- ✅ Response still generated (without retrieval)
- ✅ No crash or blank screen

**How to Test:**
1. Check backend logs for ChromaDB connection
2. Temporarily rename ChromaDB data folder or stop service
3. Send query
4. Verify graceful fallback
5. Restore ChromaDB

---

### Task 13: Cross-Browser Testing

#### Browsers to Test

- [ ] **Chrome (Desktop)** - macOS/Windows/Linux
- [ ] **Safari (Desktop)** - macOS
- [ ] **Firefox** - macOS/Windows/Linux
- [ ] **Chrome (Mobile)** - Android/iOS
- [ ] **Safari (iOS)** - iPhone/iPad

#### Test Checklist (All Browsers)

For each browser, run through:

1. **SSE Streaming:**
   - ✅ EventSource connects successfully
   - ✅ Events stream without errors
   - ✅ UI updates in real-time
   - ✅ Connection closes properly

2. **Visual Rendering:**
   - ✅ All panels display correctly
   - ✅ Agent cards render properly
   - ✅ Execution graph displays
   - ✅ Retrieval panel shows documents
   - ✅ No visual regressions (compare to Chrome)

3. **Interactive Features:**
   - ✅ Send message works
   - ✅ Document modal opens/closes
   - ✅ Panel collapse works
   - ✅ Scroll areas function
   - ✅ Tooltips display

4. **Performance:**
   - ✅ No lag or freezing
   - ✅ Smooth animations
   - ✅ Fast initial load

#### Known Browser-Specific Issues

**Safari:**
- SSE reconnection may behave differently
- Check for any EventSource polyfill needs

**Mobile Chrome/Safari:**
- Check touch interactions (modal close, scroll)
- Verify responsive layout
- Test on both phone and tablet sizes

**Firefox:**
- DevTools Network tab shows SSE differently
- Verify events still process correctly

#### How to Test

1. Open app in each browser
2. Run through basic test flow:
   - Send query: "What is RAG?"
   - Verify all panels update
   - Open document modal
   - Check execution graph
3. Check DevTools console for errors
4. Document any browser-specific issues

---

## Test Results Template

### Test Summary

| Task | Status | Notes |
|------|--------|-------|
| Task 2: AgentPanel | ⬜ PASS / ⬜ FAIL | |
| Task 3: AgentMetrics | ⬜ PASS / ⬜ FAIL | |
| Task 4: RetrievalPanel | ⬜ PASS / ⬜ FAIL | |
| Task 5: ExecutionGraph | ⬜ PASS / ⬜ FAIL | |
| Task 6: DocumentModal | ⬜ PASS / ⬜ FAIL | |
| Task 7: CacheCard | ⬜ PASS / ⬜ FAIL | |
| Task 8: Specialized Cards | ⬜ PASS / ⬜ FAIL | |
| Task 9: ChatProvider | ⬜ PASS / ⬜ FAIL | |
| Task 10: SSE Streaming | ⬜ PASS / ⬜ FAIL | |
| Task 11: Environment | ⬜ PASS / ⬜ FAIL | |
| Task 12: All Agent Types | ⬜ PASS / ⬜ FAIL | |
| Task 13: Cross-Browser | ⬜ PASS / ⬜ FAIL | |

### Issues Found

| Issue # | Task | Severity | Description | Steps to Reproduce |
|---------|------|----------|-------------|-------------------|
| 1 | | ⬜ Critical / ⬜ High / ⬜ Medium / ⬜ Low | | |
| 2 | | ⬜ Critical / ⬜ High / ⬜ Medium / ⬜ Low | | |

### Browser Compatibility

| Browser | Version | Status | Issues |
|---------|---------|--------|--------|
| Chrome (Desktop) | | ⬜ PASS / ⬜ FAIL | |
| Safari (Desktop) | | ⬜ PASS / ⬜ FAIL | |
| Firefox | | ⬜ PASS / ⬜ FAIL | |
| Chrome (Mobile) | | ⬜ PASS / ⬜ FAIL | |
| Safari (iOS) | | ⬜ PASS / ⬜ FAIL | |

### Overall Assessment

**Ready for Production?** ⬜ YES / ⬜ NO

**Reason:**
<!-- Explain why ready or not ready -->

---

## Troubleshooting

### Backend Not Responding

**Symptoms:**
- "Network error" in frontend
- No SSE connection in DevTools

**Solutions:**
1. Check backend is running: `curl http://localhost:8000/health`
2. Check logs in backend terminal
3. Restart backend: `Ctrl+C` then `python -m uvicorn...`

### SSE Connection Fails

**Symptoms:**
- "SSE connection failed" in console
- No real-time updates

**Solutions:**
1. Check CORS settings in backend
2. Verify `/api/chat/stream/*` routes work
3. Check browser EventSource support
4. Try different browser

### Components Show Mock Data

**Symptoms:**
- Values like "1000 tokens", "$0.00", "0ms"
- Not changing between queries

**Solutions:**
1. Verify backend sending real data
2. Check ChatProvider receiving events
3. Check console for API errors
4. Verify `NEXT_PUBLIC_API_URL` set correctly

### Documents Not Appearing

**Symptoms:**
- Retrieval panel empty
- No documents in modal

**Solutions:**
1. Verify ChromaDB running and populated
2. Check backend logs for retrieval errors
3. Verify query triggers retrieval (not guide mode)
4. Check `retrieval_log` events in DevTools

---

## Next Steps After Manual QA

1. **Document all findings** in test results template
2. **Create tickets** for any issues found (Critical/High priority)
3. **Update story status** to "Ready for Review" if all pass
4. **Notify QA agent** to run automated regression (if applicable)
5. **Prepare demo** for stakeholders showing all 5 agent types working

---

**Testing completed by:** _________________

**Date:** _________________

**Sign-off:** _________________
