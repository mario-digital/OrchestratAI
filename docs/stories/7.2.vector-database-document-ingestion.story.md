# <!-- Powered by BMAD™ Core -->

## Status
Draft

## Story
**As a** backend developer,
**I want** a ChromaDB vector store with document ingestion pipeline,
**so that** RAG agents can retrieve relevant documents using semantic similarity search

## Acceptance Criteria
1. `docker compose up chromadb` succeeds with healthy status
2. Running `uv run python scripts/ingest_data.py --source data/docs` ingests the sample corpus without errors
3. `ChromaVectorStore.similarity_search` returns documents with metadata populated (source + page)
4. `ChromaVectorStore.similarity_search_with_scores` returns cosine scores consistent with LangChain expectations (< 0.5 for strong matches)
5. Tests in `tests/retrieval` pass with ≥90% coverage and clean up temporary directories
6. README snippet documents how to add new documents and re-run ingestion

## Tasks / Subtasks

- [ ] Task 1: Container + configuration (AC: 1)
  - [ ] Extend `docker-compose.yml` with ChromaDB service
  - [ ] Set container name: `orchestratai-chromadb`
  - [ ] Expose port `8001` (map internal 8000 → external 8001 to avoid FastAPI conflict)
  - [ ] Configure persistent volume: `chroma_data:/chroma/chroma`
  - [ ] Set environment variables: `IS_PERSISTENT=TRUE`, `ANONYMIZED_TELEMETRY=FALSE`
  - [ ] Add to `orchestratai-network`
  - [ ] Configure health check: `curl -f http://localhost:8000/api/v1/heartbeat`
  - [ ] Add volume definition to docker-compose.yml
  - [ ] Test `docker compose up chromadb` starts successfully
  - [ ] Verify health check: `curl http://localhost:8001/api/v1/heartbeat`

- [ ] Task 2: Vector store abstraction (AC: 3, 4)
  - [ ] Create `orchestratai_api/src/retrieval/__init__.py`
  - [ ] Create `orchestratai_api/src/retrieval/vector_store.py`
  - [ ] Define `VectorStore` ABC protocol
  - [ ] Add abstract method `add_documents(documents: list[Document]) -> None`
  - [ ] Add abstract method `similarity_search(query: str, k: int = 5) -> list[Document]`
  - [ ] Add abstract method `similarity_search_with_scores(query: str, k: int = 5) -> list[tuple[Document, float]]`
  - [ ] Add abstract method `clear() -> None`
  - [ ] Use async-friendly signatures (wrap sync Chroma calls with `anyio.to_thread.run_sync`)
  - [ ] Import `Document` from `langchain_core.documents`

- [ ] Task 3: Chroma implementation (AC: 3, 4)
  - [ ] Create `orchestratai_api/src/retrieval/chroma_store.py`
  - [ ] Import `Chroma` from `langchain_community.vectorstores`
  - [ ] Import `OpenAIEmbeddings` from `langchain_openai`
  - [ ] Implement `ChromaVectorStore(VectorStore)` class
  - [ ] Initialize embeddings with `text-embedding-3-large` model
  - [ ] Initialize Chroma client with `persist_directory` parameter (default: `/app/chroma_db`)
  - [ ] Implement `add_documents` method
  - [ ] Implement `similarity_search` method
  - [ ] Implement `similarity_search_with_scores` method (returns scores < 0.5 for strong matches)
  - [ ] Implement `clear` method
  - [ ] Support search modes: `mmr`, `score_threshold`
  - [ ] Wrap sync Chroma operations with `anyio.to_thread.run_sync` for async compatibility

- [ ] Task 4: Ingestion pipeline (AC: 2)
  - [ ] Create `scripts/ingest_data.py`
  - [ ] Import loaders: `DirectoryLoader`, `PyPDFLoader` from `langchain_community.document_loaders`
  - [ ] Import `RecursiveCharacterTextSplitter` from `langchain_text_splitters`
  - [ ] Implement CLI with argparse: `--source`, `--collection`, `--chunk-size`, `--chunk-overlap`, `--reset`
  - [ ] Default parameters: `chunk_size=512`, `chunk_overlap=80`
  - [ ] Support file types: Markdown (.md), JSON (.json), PDF (.pdf)
  - [ ] Load documents from source directory
  - [ ] Split documents with `RecursiveCharacterTextSplitter`
  - [ ] Add metadata to each chunk: `source`, `page`, `ingested_at`
  - [ ] Embed and persist to ChromaVectorStore
  - [ ] Support `--reset` flag to clear collection before ingestion
  - [ ] Print progress: document count, chunk count, embedding progress
  - [ ] Handle errors gracefully with helpful messages

- [ ] Task 5: Sample corpus (AC: 2, 6)
  - [ ] Create `data/docs/` directory at repo root
  - [ ] Seed with at least 50 documents (mix of FAQs, policies, API guides)
  - [ ] Include sample PDFs or convert Markdown to PDF for testing
  - [ ] Document corpus structure in README
  - [ ] Add `.gitkeep` or sample files to `data/docs`
  - [ ] Document how to add new documents
  - [ ] Document how to refresh the store: `uv run python scripts/ingest_data.py --source data/docs --reset`

- [ ] Task 6: Integration tests (AC: 5)
  - [ ] Create `orchestratai_api/tests/retrieval/__init__.py`
  - [ ] Create `orchestratai_api/tests/retrieval/test_chroma_store.py`
  - [ ] Use `pytest` + `pytest-asyncio`
  - [ ] Create temporary Chroma instance in `tmp_path` fixture
  - [ ] Test document ingestion (validate document count)
  - [ ] Test similarity search (validate ordering by relevance)
  - [ ] Test similarity search with scores (validate score range 0.0-1.0)
  - [ ] Test MMR search mode
  - [ ] Test score threshold filtering
  - [ ] Test metadata population (source + page)
  - [ ] Test clear operation
  - [ ] Ensure embeddings generated via provider factory's embedding role
  - [ ] Clean up temporary directories after tests
  - [ ] Achieve ≥90% coverage

## Dev Notes

### Architecture Context
**[Source: docs/agent-planning/agent-feature-planning.md, Story 7.2]**

Bootstrap the retrieval layer following LangChain learning repository patterns. We need a Chroma container for development, a thin abstraction so we can swap Pinecone later, and an ingestion script capable of loading PDFs/Markdown into the store.

### Docker Compose Configuration
**[Source: agent-feature-planning.md:274-328]**

Add to existing `docker-compose.yml`:

```yaml
services:
  # Existing services: backend, frontend, redis

  # NEW: ChromaDB for vector storage
  chromadb:
    image: chromadb/chroma:latest
    container_name: orchestratai-chromadb
    ports:
      - "8001:8000"  # Expose on 8001 (avoid conflict with FastAPI on 8000)
    volumes:
      - chroma_data:/chroma/chroma
    environment:
      - IS_PERSISTENT=TRUE
      - ANONYMIZED_TELEMETRY=FALSE
    networks:
      - orchestratai-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/heartbeat"]
      interval: 30s
      timeout: 10s
      retries: 3

volumes:
  chroma_data:
    driver: local
  # ... existing volumes (postgres_data, redis_data)
```

### Vector Store Abstraction
**[Source: agent-feature-planning.md:621-644]**

```python
from abc import ABC, abstractmethod
from langchain_core.documents import Document

class VectorStore(ABC):
    @abstractmethod
    async def add_documents(self, *, documents: list[Document]) -> None: ...

    @abstractmethod
    async def similarity_search(self, *, query: str, k: int = 5) -> list[Document]: ...

    @abstractmethod
    async def similarity_search_with_scores(self, *, query: str, k: int = 5) -> list[tuple[Document, float]]: ...

    @abstractmethod
    async def clear(self) -> None: ...
```

Use async-friendly signatures even though Chroma's SDK is sync (wrap with `anyio.to_thread.run_sync`).

### Chroma Implementation
**[Source: agent-feature-planning.md:646-661]**

```python
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings

class ChromaVectorStore(VectorStore):
    def __init__(self, *, persist_directory: str = "/app/chroma_db"):
        self._embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
        self._client = Chroma(
            persist_directory=persist_directory,
            embedding_function=self._embeddings,
        )
```

Expose helpers for similarity search modes (`mmr`, `score_threshold`) so we can switch strategies from the agents.

### Ingestion Pipeline
**[Source: agent-feature-planning.md:663-674]**

Port the end-to-end flow from the LangChain repo's PDF demo: load → split → embed → persist.

Support Markdown, JSON, and PDF by composing loaders (`DirectoryLoader`, `PyPDFLoader`) and `RecursiveCharacterTextSplitter` with `chunk_size=512`, `chunk_overlap=80`.

Remember to respect `USE_ONEPASSWORD=true` (default) so embedding API keys resolve via 1Password unless explicitly disabled.

CLI usage:
```bash
uv run python scripts/ingest_data.py \
  --source ./data/docs \
  --collection knowledge_base_v1 \
  --chunk-size 512 \
  --chunk-overlap 80
```

Record metadata (`source`, `page`, `ingested_at`) on each `Document`.

### Environment Variables
**[Source: agent-feature-planning.md:332-386]**

Add to `orchestratai_api/.env`:

```bash
# Vector DB
VECTOR_DB_BACKEND=chromadb  # or 'pinecone' for production
CHROMADB_HOST=chromadb
CHROMADB_PORT=8000
PINECONE_API_KEY=...  # for future production use
PINECONE_ENVIRONMENT=...
```

### Python Dependencies
**[Source: agent-feature-planning.md:388-418]**

Add to `orchestratai_api/pyproject.toml`:

```toml
dependencies = [
    # NEW: Vector DB & Retrieval
    "chromadb>=0.4.22",
    "sentence-transformers>=2.3.0",  # Optional: local embeddings
    "langchain-community>=0.0.20",
]
```

### File Structure
**[Source: architecture/source-tree.md]**

```
orchestratai_api/
├── src/
│   └── retrieval/
│       ├── __init__.py
│       ├── vector_store.py        # NEW: VectorStore ABC
│       └── chroma_store.py        # NEW: ChromaVectorStore implementation
├── scripts/
│   ├── __init__.py
│   └── ingest_data.py             # NEW: Document ingestion CLI
├── data/
│   └── docs/                      # NEW: Sample corpus directory
│       ├── .gitkeep
│       └── (50+ documents)
└── tests/
    └── retrieval/
        ├── __init__.py
        └── test_chroma_store.py   # NEW: Integration tests
```

### Testing

**[Source: architecture/15-testing-strategy.md]**

**Test File Locations:**
- `orchestratai_api/tests/retrieval/test_chroma_store.py`

**Testing Framework:**
- pytest for all backend tests
- pytest-asyncio for async vector store methods
- Temporary Chroma instances using `tmp_path` fixture
- Clean up test directories after execution

**Test Coverage Requirements:**
- Minimum 90% coverage required
- Test document ingestion and count
- Test similarity search ordering
- Test similarity search with scores
- Test MMR behavior
- Test score-threshold filtering
- Validate metadata population

**Example Test Structure:**
```python
import pytest
from pathlib import Path
from langchain_core.documents import Document

@pytest.mark.asyncio
async def test_ingest_and_search(tmp_path: Path):
    store = ChromaVectorStore(persist_directory=tmp_path.as_posix())
    doc = Document(page_content="Retrieval-Augmented Generation improves answers", metadata={"source": "demo.md"})
    await store.add_documents(documents=[doc])

    results = await store.similarity_search(query="What is RAG?", k=1)
    assert results and results[0].metadata["source"] == "demo.md"
```

### Sample Corpus Guidelines
**[Source: agent-feature-planning.md:675-678]**

Seed `data/docs/` with at least 50 documents:
- Mix of pricing FAQs
- Policy PDFs
- API guides
- Technical documentation

For PDFs, reuse existing samples or convert Markdown to PDF for testing.

### ChromaDB Connection
**[Source: agent-feature-planning.md:346-353]**

```bash
# Vector DB configuration
VECTOR_DB_BACKEND=chromadb
CHROMADB_HOST=chromadb  # Docker service name
CHROMADB_PORT=8000      # Internal port (exposed as 8001)
```

When running in Docker: connect to `chromadb:8000`
When running locally: connect to `localhost:8001`

### Migration Path to Pinecone
**[Source: agent-feature-planning.md:47-52]**

Start with ChromaDB (Docker, local dev). Clean abstraction allows easy Pinecone migration later:
- Implement new `PineconeVectorStore(VectorStore)` class
- Update `VECTOR_DB_BACKEND=pinecone` in .env
- No changes to agent code (uses `VectorStore` abstraction)

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-11-02 | 1.0 | Initial story creation | Bob (Scrum Master) |

## Dev Agent Record

### Agent Model Used
(To be filled by dev agent)

### Debug Log References
(To be filled by dev agent)

### Completion Notes List
(To be filled by dev agent)

### File List
(To be filled by dev agent)

## QA Results
(To be filled by QA agent)
