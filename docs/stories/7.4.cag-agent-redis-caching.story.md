# <!-- Powered by BMAD™ Core -->

## Status
Draft

## Story
**As a** backend developer,
**I want** a Cached-Augmented Generation (CAG) agent with semantic caching in Redis,
**so that** repeated queries return instantly with zero cost while maintaining response quality

## Acceptance Criteria
1. First request for a policy/pricing question triggers Bedrock Haiku and persists the result in Redis
2. Subsequent similar queries return from cache with cost = 0 and latency < 500 ms
3. SSE stream includes `cache_hit` data in retrieval logs and metrics
4. Orchestrator routing reflects CAG usage (guide vs delegate vs CAG)
5. Automated tests in `tests/cag` pass with ≥90% coverage

## Tasks / Subtasks

- [ ] Task 1: Semantic cache implementation (AC: 1, 2)
  - [ ] Create `orchestratai_api/src/cache/__init__.py`
  - [ ] Create `orchestratai_api/src/cache/redis_cache.py`
  - [ ] Import `redis.asyncio` for async Redis operations
  - [ ] Implement `RedisSemanticCache` class
  - [ ] Add `__init__` with TTL parameter (default: 3600 seconds)
  - [ ] Connect to Redis using `resolve_secret("REDIS_HOST")` (falls back to `.env` when `USE_ONEPASSWORD=false`)
  - [ ] Implement `async def get(embedding: list[float], threshold: float = 0.85) -> tuple[dict | None, float]`
  - [ ] Store cache as list: `cag_cache` key
  - [ ] Each item: `{"embedding": [...], "payload": {...}}`
  - [ ] Compute cosine similarity for each cached item
  - [ ] Return payload if similarity >= threshold
  - [ ] Implement `async def set(embedding: list[float], payload: dict) -> None`
  - [ ] Append to cache list with `lpush`
  - [ ] Trim list to max 1000 items (LRU approximation)
  - [ ] Add helper: `cosine_similarity(vec1: list[float], vec2: list[float]) -> float`
  - [ ] Use numpy for efficient computation

- [ ] Task 2: CAG agent implementation (AC: 1, 2, 3)
  - [ ] Create `orchestratai_api/src/agents/workers/cag_agent.py`
  - [ ] Extend `BaseAgent` from Story 7.3
  - [ ] Inject `RedisSemanticCache`, embeddings provider (via `ProviderFactory` EMBEDDINGS role), and Bedrock Haiku provider (via `ProviderFactory` CAG role)
  - [ ] Implement workflow:
    - Embed query using embeddings provider (text-embedding-3-large)
    - Call `cache.get(embedding=embedding, threshold=0.85)`
    - If cache hit: Return cached response with `cache_hit=True` in metrics
    - If cache miss: Call Bedrock Haiku provider
    - On miss: Persist to cache with `cache.set(embedding, payload)`
  - [ ] Payload structure: `{"message": str, "metrics": dict, "timestamp": float}`
  - [ ] Return `ChatResponse` with message
  - [ ] Return `AgentMetrics` with `cache_hit` flag
  - [ ] On cache hit: Set cost=0, latency=cache_lookup_time
  - [ ] On cache miss: Include full Haiku cost and latency
  - [ ] Log cache operation to `retrieval_logs` (type: CACHE)

- [ ] Task 3: Routing updates (AC: 4)
  - [ ] Update `orchestratai_api/src/agents/orchestrator.py`
  - [ ] Extend intent classification in `analyse_query`
  - [ ] Add intent types: POLICY_QUESTION, PRICING_QUESTION
  - [ ] Update `decide_route` logic:
    - If POLICY_QUESTION or PRICING_QUESTION → route to CAG
    - If DOMAIN_QUESTION → route to RAG
    - If META_QUESTION → guide mode
  - [ ] Update `delegate_to_worker` to fetch the CAG agent instance from the dependency container / ProviderFactory when needed
  - [ ] Ensure state captures `route="cag"` and selected agent id

- [ ] Task 4: Observability (AC: 3)
  - [ ] Update retrieval log schema (`src/models/schemas.py`) to support cache logs
  - [ ] Emit `LogType.CACHE` logs with:
    - `operation`: "hit" or "miss"
    - `latency_ms`: Cache lookup time
    - `similarity_score`: For hits, the cosine similarity
    - `saved_cost`: For hits, the cost saved by not calling LLM
  - [ ] Update `AgentMetrics` (`src/models/schemas.py`) to include `cache_hit: bool` field
  - [ ] In `AgentService.process_chat_stream`, emit cache logs as `retrieval_log` events
  - [ ] Ensure metrics show Haiku cost/tokens even on cache hits (zeros)

- [ ] Task 5: Unit tests (AC: 5)
  - [ ] Create `tests/cag/__init__.py`
  - [ ] Create `tests/cag/test_redis_cache.py`
  - [ ] Mock Redis client
  - [ ] Test cache miss: `get` returns None
  - [ ] Test cache hit: `get` returns cached payload
  - [ ] Test cosine similarity calculation
  - [ ] Test similarity threshold (0.85)
  - [ ] Test cache eviction (list trimmed to 1000)
  - [ ] Test TTL setting
  - [ ] Create `tests/cag/test_cag_agent.py`
  - [ ] Mock RedisSemanticCache
  - [ ] Mock embeddings provider
  - [ ] Mock Bedrock Haiku provider
  - [ ] Test workflow on cache miss:
    - Verify embedding generated
    - Verify cache lookup
    - Verify provider called
    - Verify cache set
    - Verify metrics show cache_hit=False
  - [ ] Test workflow on cache hit:
    - Verify cache returns payload
    - Verify provider NOT called
    - Verify metrics show cache_hit=True, cost=0
  - [ ] Test cache log emission

- [ ] Task 6: Integration tests (AC: 5)
  - [ ] Create `tests/cag/test_cag_integration.py`
  - [ ] Use real Redis instance (test container or local)
  - [ ] Test end-to-end flow:
    - First query: "Can I get a refund?"
    - Verify cache miss
    - Verify Haiku called
    - Verify response cached
  - [ ] Send same query again:
    - Verify cache hit
    - Verify cost=0
    - Verify latency < 500ms
  - [ ] Test orchestrator routing:
    - Send policy question
    - Verify routed to CAG agent
    - Verify not routed to RAG
  - [ ] Test SSE streaming includes cache logs via `AgentService.process_chat_stream`
  - [ ] Clean up Redis test data after tests
  - [ ] Achieve ≥90% coverage

## Dev Notes

### Architecture Context
**[Source: docs/agent-planning/agent-feature-planning.md, Story 7.4]**

Introduce the Cached-Augmented Generation (CAG) worker. It mirrors the LangChain repo's semantic caching approach: compute an embedding for the query, look up a near neighbor in Redis, and fall back to generation when the cache misses. This keeps repeated/policy questions cheap by routing them to Claude 3 Haiku.

### Semantic Cache Strategy
**[Source: agent-feature-planning.md:813-850]**

Semantic caching differs from exact-match caching:
- **Traditional cache**: "Can I get refund?" ≠ "Can I get a refund?" (cache miss)
- **Semantic cache**: Both queries have similar embeddings → cache hit

Flow:
1. Embed query → vector
2. Compare with cached query vectors (cosine similarity)
3. If similarity ≥ 0.85 → return cached response
4. Otherwise → generate new response + cache it

### RedisSemanticCache Implementation
**[Source: agent-feature-planning.md:826-850]**

```python
import json
import numpy as np
import redis.asyncio as redis

class RedisSemanticCache:
    def __init__(self, *, ttl_seconds: int = 3600):
        self._client = redis.from_url("redis://" + resolve_secret("REDIS_HOST"))
        self._ttl = ttl_seconds

    async def get(self, *, embedding: list[float], threshold: float = 0.85):
        items = await self._client.lrange("cag_cache", 0, -1)
        for raw in items:
            record = json.loads(raw)
            score = cosine_similarity(record["embedding"], embedding)
            if score >= threshold:
                return record["payload"], score
        return None, 0.0

    async def set(self, *, embedding: list[float], payload: dict) -> None:
        await self._client.lpush("cag_cache", json.dumps({"embedding": embedding, "payload": payload}))
        await self._client.ltrim("cag_cache", 0, 999)
```

Reuse the embedding provider from Story 7.1 so cache keys align with vector store.

### CAG Agent Implementation
**[Source: agent-feature-planning.md:852-863]**

```python
class CAGAgent(BaseAgent):
    def __init__(self, *, provider: BaseLLMProvider, cache: RedisSemanticCache, embeddings: BaseLLMProvider):
        super().__init__(role=AgentRole.CAG, provider=provider)
        self._cache = cache
        self._embeddings = embeddings

    async def run(self, request: ChatRequest, **kwargs) -> ChatResponse:
        start = time.perf_counter()

        # Embed query
        query_embedding = await self._embeddings.embed(request.message)

        # Check cache
        cached, score = await self._cache.get(embedding=query_embedding)
        if cached:
            # Cache hit - return immediately
            return self._build_response(cached, cache_hit=True, similarity=score)

        # Cache miss - call provider
        result = await self.provider.complete(messages=[{"role": "user", "content": request.message}])

        # Persist to cache
        payload = {"message": result.content, "metrics": {...}, "timestamp": time.time()}
        await self._cache.set(embedding=query_embedding, payload=payload)

        return self._build_response(result, cache_hit=False)
```

Output `AgentMetrics` with `cache_hit` flag and cost savings.

### Routing Updates
**[Source: agent-feature-planning.md:865-870]**

Extend orchestrator decision logic to send policy/pricing questions to CAG agent:

```python
def decide_route(state: OrchestratorState) -> str:
    intent = state["analysis"]["intent"]

    if intent in ["META_QUESTION", "ROUTING_QUESTION"]:
        return "guide"
    elif intent in ["POLICY_QUESTION", "PRICING_QUESTION"]:
        return "delegate_cag"  # NEW: Route to CAG
    elif intent == "DOMAIN_QUESTION":
        return "delegate_rag"
    else:
        return "delegate_direct"
```

Record fallback path so frontend can display when CAG short-circuits response.

### Observability
**[Source: agent-feature-planning.md:872-881]**

Emit `cache` retrieval logs mirroring mock implementation (`LogType.CACHE`):

```python
{
  "type": "CACHE",
  "operation": "hit",  # or "miss"
  "latency_ms": 45,
  "similarity_score": 0.92,
  "saved_cost": 0.0003,  # Cost saved by not calling Haiku
  "timestamp": 1234567890.123
}
```

Include hit/miss status and effective latency.

Update `metrics.agent_status` to show Haiku cost/token usage even on cache hits (zeros when served from cache).

### Cost Savings Example
**[Source: agent-feature-planning.md:211-266]**

Bedrock Claude 3 Haiku pricing:
- Input: $0.25 / 1M tokens
- Output: $1.25 / 1M tokens

Typical policy query:
- Input: 100 tokens
- Output: 300 tokens
- Cost: (100/1M × $0.25) + (300/1M × $1.25) = $0.000025 + $0.000375 = **$0.0004**

With 70% cache hit rate:
- 70% of queries: $0 (cache)
- 30% of queries: $0.0004 (Haiku)
- **Average: $0.00012 per query** (70% cost reduction)

### File Structure
**[Source: architecture/source-tree.md]**

```
orchestratai_api/src/
├── cache/
│   ├── __init__.py
│   └── redis_cache.py         # NEW: RedisSemanticCache
├── agents/
│   └── workers/
│       └── cag_agent.py       # NEW: CAG agent

orchestratai_api/tests/cag/
├── __init__.py
├── test_redis_cache.py        # NEW: Cache tests
├── test_cag_agent.py          # NEW: CAG agent tests
└── test_cag_integration.py    # NEW: End-to-end tests
```

### Testing

**[Source: architecture/15-testing-strategy.md]**

**Test File Locations:**
- `orchestratai_api/tests/cag/test_redis_cache.py`
- `orchestratai_api/tests/cag/test_cag_agent.py`
- `orchestratai_api/tests/cag/test_cag_integration.py`

**Testing Framework:**
- pytest + pytest-asyncio
- Mock Redis for unit tests
- Real Redis for integration tests (test container)
- Mock embeddings and provider responses

**Test Coverage Requirements:**
- Minimum 90% coverage required
- Test cache hit/miss branches
- Test similarity threshold
- Test cost calculation
- Integration test: same query twice → second is cached

**Example Test Structure:**
```python
import pytest

@pytest.mark.asyncio
async def test_cag_cache_hit(cag_agent):
    first = await cag_agent.run("Can I get a refund?", session_id="abc")
    second = await cag_agent.run("Can I get a refund?", session_id="abc")
    assert second.metrics.cache_hit is True
    assert second.metrics.cost_total == 0
```

### Environment Variables
**[Source: agent-feature-planning.md:332-386]**

```bash
# Redis (already configured in Story 5.5)
REDIS_HOST=redis
REDIS_PORT=6379

# CAG agent model
DEFAULT_CAG_MODEL=anthropic.claude-3-haiku-20240307-v1:0

# Cache configuration
CACHE_TTL=3600  # 1 hour
SIMILARITY_THRESHOLD=0.85
```

### Python Dependencies
**[Source: agent-feature-planning.md:388-418]**

Ensure these are in `pyproject.toml`:

```toml
dependencies = [
    # Redis (already added in Story 5.5)
    "redis>=5.0.0",

    # Utilities
    "numpy>=1.24.0",  # For cosine similarity
]
```

### Cosine Similarity Calculation

```python
import numpy as np

def cosine_similarity(vec1: list[float], vec2: list[float]) -> float:
    """Compute cosine similarity between two vectors."""
    a = np.array(vec1)
    b = np.array(vec2)
    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))
```

Returns value 0.0 - 1.0:
- 1.0 = identical vectors
- 0.85+ = very similar (cache hit threshold)
- < 0.85 = different enough (cache miss)

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-11-02 | 1.0 | Initial story creation | Bob (Scrum Master) |

## Dev Agent Record

### Agent Model Used
(To be filled by dev agent)

### Debug Log References
(To be filled by dev agent)

### Completion Notes List
(To be filled by dev agent)

### File List
(To be filled by dev agent)

## QA Results
(To be filled by QA agent)
